{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alf1bmNV80el",
    "outputId": "38b67413-01ec-400a-ed44-749e5843208f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Flowers102\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
    "])\n",
    "\n",
    "# Download the dataset (training split)\n",
    "train_dataset = Flowers102(\n",
    "    root='./data',  # The root directory where the dataset will be saved\n",
    "    split='train',   # 'train' for the training set, 'test' for the test set\n",
    "    transform=transform,  # Apply the defined transformation\n",
    "    download=True  # Download if not already present\n",
    ")\n",
    "\n",
    "# Download the dataset (validation split)\n",
    "val_dataset = Flowers102(\n",
    "    root='./data',  # The root directory where the dataset will be saved\n",
    "    split='val',   # 'train' for the training set, 'test' for the test set\n",
    "    transform=transform,  # Apply the defined transformation\n",
    "    download=True  # Download if not already present\n",
    ")\n",
    "\n",
    "# Download the dataset (test split)\n",
    "test_dataset = Flowers102(\n",
    "    root='./data',  # The root directory where the dataset will be saved\n",
    "    split='test',   # 'train' for the training set, 'test' for the test set\n",
    "    transform=transform,  # Apply the defined transformation\n",
    "    download=True  # Download if not already present\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjoxsUobdlrU",
    "outputId": "574c5311-2b42-44e1-9602-9a585243c1e1"
   },
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Unan_EoldpOp",
    "outputId": "376b8f77-4bf7-445b-d0cf-a5edf44bc8ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a function to generate attention masks using a Faster R-CNN model\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.5\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "z3HUdXnDjXqE"
   },
   "outputs": [],
   "source": [
    "# Define a new model that combines ResNet50 and Faster R-CNN with attention masks\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "v3d66WNojqhl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 145.8381 | Val Loss: 150.6731\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 1.5686%\n",
      "Epoch 2/10 | Train Loss: 145.4570 | Val Loss: 150.4869\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 1.3725%\n",
      "Epoch 3/10 | Train Loss: 144.5057 | Val Loss: 152.0694\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 1.2745%\n",
      "Epoch 4/10 | Train Loss: 144.2106 | Val Loss: 154.2007\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 0.9804%\n",
      "Epoch 5/10 | Train Loss: 143.3927 | Val Loss: 154.5563\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 1.0784%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ESd1PfvPj17Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0128\n",
      "Test Loss: 922.3398\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 154.4638 | Val Loss: 1740.7152\n",
      "Train Accuracy: 0.4902% | Val Accuracy: 1.2745%\n",
      "Epoch 2/10 | Train Loss: 147.3857 | Val Loss: 242.3766\n",
      "Train Accuracy: 0.7843% | Val Accuracy: 1.4706%\n",
      "Epoch 3/10 | Train Loss: 147.0735 | Val Loss: 152.7366\n",
      "Train Accuracy: 0.6863% | Val Accuracy: 1.1765%\n",
      "Epoch 4/10 | Train Loss: 146.3994 | Val Loss: 153.8204\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 1.3725%\n",
      "Epoch 5/10 | Train Loss: 145.9526 | Val Loss: 150.0019\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 1.4706%\n",
      "Epoch 6/10 | Train Loss: 144.8736 | Val Loss: 154.1641\n",
      "Train Accuracy: 1.1765% | Val Accuracy: 1.0784%\n",
      "Epoch 7/10 | Train Loss: 144.2154 | Val Loss: 154.2321\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 1.2745%\n",
      "Epoch 8/10 | Train Loss: 143.3090 | Val Loss: 159.6637\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 0.8824%\n",
      "Epoch 9/10 | Train Loss: 142.2424 | Val Loss: 158.7973\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 0.7843%\n",
      "Epoch 10/10 | Train Loss: 141.7401 | Val Loss: 159.2855\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 0.9804%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 945.1780\n",
      "Test Accuracy: 1.0246%\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold in generate_attention_masks function = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetWithAttention(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rcnn): FasterRCNN(\n",
       "    (transform): GeneralizedRCNNTransform(\n",
       "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "        Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "    )\n",
       "    (backbone): BackboneWithFPN(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fpn): FeaturePyramidNetwork(\n",
       "        (inner_blocks): ModuleList(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (layer_blocks): ModuleList(\n",
       "          (0-3): 4 x Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (extra_blocks): LastLevelMaxPool()\n",
       "      )\n",
       "    )\n",
       "    (rpn): RegionProposalNetwork(\n",
       "      (anchor_generator): AnchorGenerator()\n",
       "      (head): RPNHead(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (roi_heads): RoIHeads(\n",
       "      (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "      (box_head): TwoMLPHead(\n",
       "        (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (box_predictor): FastRCNNPredictor(\n",
       "        (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=102, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 147.7849 | Val Loss: 147.5459\n",
      "Train Accuracy: 0.6863% | Val Accuracy: 1.1765%\n",
      "Epoch 2/10 | Train Loss: 145.5919 | Val Loss: 147.4417\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 1.0784%\n",
      "Epoch 3/10 | Train Loss: 143.3535 | Val Loss: 149.3132\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 0.8824%\n",
      "Epoch 4/10 | Train Loss: 141.4230 | Val Loss: 153.9575\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 1.0784%\n",
      "Epoch 5/10 | Train Loss: 139.3563 | Val Loss: 154.4517\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 0.7843%\n",
      "Epoch 6/10 | Train Loss: 137.1258 | Val Loss: 156.6957\n",
      "Train Accuracy: 4.1176% | Val Accuracy: 1.3725%\n",
      "Epoch 7/10 | Train Loss: 135.0922 | Val Loss: 157.0253\n",
      "Train Accuracy: 5.1961% | Val Accuracy: 0.9804%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 967.0374\n",
      "Test Accuracy: 0.9595%\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 159.7020 | Val Loss: 148.8961\n",
      "Train Accuracy: 0.6863% | Val Accuracy: 0.8824%\n",
      "Epoch 2/10 | Train Loss: 149.3556 | Val Loss: 150.2894\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 1.3725%\n",
      "Epoch 3/10 | Train Loss: 145.7122 | Val Loss: 150.8001\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 0.8824%\n",
      "Epoch 4/10 | Train Loss: 142.9772 | Val Loss: 151.9860\n",
      "Train Accuracy: 4.0196% | Val Accuracy: 1.8627%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0098\n",
      "Test Loss: 929.8337\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold in generate_attention_masks function = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.1\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 161.9409 | Val Loss: 151.3895\n",
      "Train Accuracy: 0.7843% | Val Accuracy: 0.8824%\n",
      "Epoch 2/10 | Train Loss: 147.2397 | Val Loss: 153.2538\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 1.4706%\n",
      "Epoch 3/10 | Train Loss: 138.8354 | Val Loss: 152.8758\n",
      "Train Accuracy: 7.1569% | Val Accuracy: 1.3725%\n",
      "Epoch 4/10 | Train Loss: 133.9803 | Val Loss: 151.2581\n",
      "Train Accuracy: 8.4314% | Val Accuracy: 2.7451%\n",
      "Epoch 5/10 | Train Loss: 127.7776 | Val Loss: 153.2480\n",
      "Train Accuracy: 12.4510% | Val Accuracy: 2.6471%\n",
      "Epoch 6/10 | Train Loss: 123.0713 | Val Loss: 152.8910\n",
      "Train Accuracy: 15.4902% | Val Accuracy: 2.6471%\n",
      "Epoch 7/10 | Train Loss: 117.8200 | Val Loss: 151.5162\n",
      "Train Accuracy: 20.0000% | Val Accuracy: 3.3333%\n",
      "Epoch 8/10 | Train Loss: 114.3243 | Val Loss: 155.4261\n",
      "Train Accuracy: 20.8824% | Val Accuracy: 2.6471%\n",
      "Epoch 9/10 | Train Loss: 111.7722 | Val Loss: 154.3785\n",
      "Train Accuracy: 22.1569% | Val Accuracy: 3.6275%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 3.2851%\n",
      "Test Loss: 939.3043\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold for attention masks = 0.1. Learning rate = 0.1. Epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.1\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 376.6730 | Val Loss: 209.9411\n",
      "Train Accuracy: 1.1765% | Val Accuracy: 1.0784%\n",
      "Epoch 2/100 | Train Loss: 241.6985 | Val Loss: 216.1731\n",
      "Train Accuracy: 5.3922% | Val Accuracy: 1.1765%\n",
      "Epoch 3/100 | Train Loss: 159.6296 | Val Loss: 217.6966\n",
      "Train Accuracy: 13.8235% | Val Accuracy: 1.5686%\n",
      "Epoch 4/100 | Train Loss: 152.1220 | Val Loss: 239.6771\n",
      "Train Accuracy: 17.0588% | Val Accuracy: 3.0392%\n",
      "Epoch 5/100 | Train Loss: 141.9608 | Val Loss: 254.0207\n",
      "Train Accuracy: 20.1961% | Val Accuracy: 2.0588%\n",
      "Epoch 6/100 | Train Loss: 136.5729 | Val Loss: 270.4128\n",
      "Train Accuracy: 23.1373% | Val Accuracy: 1.9608%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 2.7647%\n",
      "Test Loss: 1618.7117\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase patience to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.1\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 376.3990 | Val Loss: 194.3790\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 0.9804%\n",
      "Epoch 2/100 | Train Loss: 222.7454 | Val Loss: 209.1663\n",
      "Train Accuracy: 5.0000% | Val Accuracy: 0.9804%\n",
      "Epoch 3/100 | Train Loss: 175.5457 | Val Loss: 224.3775\n",
      "Train Accuracy: 10.8824% | Val Accuracy: 1.8627%\n",
      "Epoch 4/100 | Train Loss: 157.3989 | Val Loss: 256.6866\n",
      "Train Accuracy: 15.9804% | Val Accuracy: 3.6275%\n",
      "Epoch 5/100 | Train Loss: 153.8353 | Val Loss: 251.2757\n",
      "Train Accuracy: 19.7059% | Val Accuracy: 2.0588%\n",
      "Epoch 6/100 | Train Loss: 139.2421 | Val Loss: 245.9022\n",
      "Train Accuracy: 25.7843% | Val Accuracy: 2.4510%\n",
      "Epoch 7/100 | Train Loss: 124.9176 | Val Loss: 252.5739\n",
      "Train Accuracy: 27.9412% | Val Accuracy: 1.7647%\n",
      "Epoch 8/100 | Train Loss: 126.6252 | Val Loss: 263.7020\n",
      "Train Accuracy: 30.2941% | Val Accuracy: 2.0588%\n",
      "Epoch 9/100 | Train Loss: 101.6075 | Val Loss: 276.0216\n",
      "Train Accuracy: 37.5490% | Val Accuracy: 2.3529%\n",
      "Epoch 10/100 | Train Loss: 106.0621 | Val Loss: 269.3032\n",
      "Train Accuracy: 35.5882% | Val Accuracy: 1.8627%\n",
      "Epoch 11/100 | Train Loss: 97.5661 | Val Loss: 274.2384\n",
      "Train Accuracy: 39.9020% | Val Accuracy: 2.1569%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 2.5045%\n",
      "Test Loss: 1591.1871\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use scheduler for learning rate. Threshold = 0.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 284.8379 | Val Loss: 166.0979\n",
      "Train Accuracy: 1.0784% | Val Accuracy: 0.9804%\n",
      "Epoch 2/100 | Train Loss: 208.9249 | Val Loss: 196.9029\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 1.1765%\n",
      "Epoch 3/100 | Train Loss: 199.7342 | Val Loss: 210.0119\n",
      "Train Accuracy: 4.6078% | Val Accuracy: 1.0784%\n",
      "Epoch 4/100 | Train Loss: 201.7185 | Val Loss: 236.9185\n",
      "Train Accuracy: 5.5882% | Val Accuracy: 1.4706%\n",
      "Epoch 5/100 | Train Loss: 188.4834 | Val Loss: 251.5518\n",
      "Train Accuracy: 5.8824% | Val Accuracy: 0.9804%\n",
      "Epoch 6/100 | Train Loss: 176.3070 | Val Loss: 237.5278\n",
      "Train Accuracy: 8.7255% | Val Accuracy: 1.7647%\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 7/100 | Train Loss: 165.9385 | Val Loss: 282.0161\n",
      "Train Accuracy: 10.4902% | Val Accuracy: 1.1765%\n",
      "Epoch 8/100 | Train Loss: 145.8363 | Val Loss: 222.6343\n",
      "Train Accuracy: 15.3922% | Val Accuracy: 1.1765%\n",
      "Epoch 9/100 | Train Loss: 119.8128 | Val Loss: 209.2375\n",
      "Train Accuracy: 21.2745% | Val Accuracy: 1.9608%\n",
      "Epoch 10/100 | Train Loss: 113.3874 | Val Loss: 205.0344\n",
      "Train Accuracy: 22.8431% | Val Accuracy: 2.0588%\n",
      "Epoch 11/100 | Train Loss: 112.8865 | Val Loss: 204.1937\n",
      "Train Accuracy: 22.6471% | Val Accuracy: 2.1569%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.6263%\n",
      "Test Loss: 1261.1616\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackle class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {}\n",
    "for inputs, labels in train_loader:\n",
    "    for label in labels:\n",
    "        if label.item() not in class_counts:\n",
    "            class_counts[label.item()] = 1\n",
    "        else:\n",
    "            class_counts[label.item()] += 1\n",
    "\n",
    "myKeys = list(class_counts.keys())\n",
    "myKeys.sort()\n",
    "sorted_dict = {i: class_counts[i] for i in myKeys}\n",
    "\n",
    "class_counts = list(sorted_dict.values())  # Example counts for three classes\n",
    "\n",
    "total_samples = sum(class_counts)\n",
    "class_weights = [count / total_samples for count in class_counts]\n",
    "\n",
    "# Convert the class weights to a PyTorch tensor\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
      "        0.0098, 0.0098, 0.0098], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sum(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 301.8348 | Val Loss: 163.7113\n",
      "Train Accuracy: 1.0784% | Val Accuracy: 1.0784%\n",
      "Epoch 2/100 | Train Loss: 216.7021 | Val Loss: 190.3002\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 1.1765%\n",
      "Epoch 3/100 | Train Loss: 197.9393 | Val Loss: 205.1313\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 1.0784%\n",
      "Epoch 4/100 | Train Loss: 190.9263 | Val Loss: 245.4285\n",
      "Train Accuracy: 5.4902% | Val Accuracy: 1.4706%\n",
      "Epoch 5/100 | Train Loss: 180.2531 | Val Loss: 241.9040\n",
      "Train Accuracy: 6.5686% | Val Accuracy: 1.5686%\n",
      "Epoch 6/100 | Train Loss: 189.5243 | Val Loss: 274.7164\n",
      "Train Accuracy: 6.9608% | Val Accuracy: 1.8627%\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 7/100 | Train Loss: 188.9300 | Val Loss: 262.0401\n",
      "Train Accuracy: 8.3333% | Val Accuracy: 1.4706%\n",
      "Epoch 8/100 | Train Loss: 140.1582 | Val Loss: 213.0095\n",
      "Train Accuracy: 16.0784% | Val Accuracy: 1.6667%\n",
      "Epoch 9/100 | Train Loss: 117.4177 | Val Loss: 202.7206\n",
      "Train Accuracy: 21.5686% | Val Accuracy: 2.4510%\n",
      "Epoch 10/100 | Train Loss: 114.3318 | Val Loss: 201.2071\n",
      "Train Accuracy: 21.9608% | Val Accuracy: 2.2549%\n",
      "Epoch 11/100 | Train Loss: 112.9491 | Val Loss: 210.4454\n",
      "Train Accuracy: 23.5294% | Val Accuracy: 2.3529%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.4962%\n",
      "Test Loss: 1308.2625\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations for data augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),  # You can adjust the rotation angle\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create data loaders with data augmentation\n",
    "train_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    transform=data_transforms['train'],  # Apply data augmentation to the training set\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='val',\n",
    "    transform=data_transforms['val'],  # Use the validation data transformation\n",
    "    download=True\n",
    ")\n",
    "\n",
    "\n",
    "# Download the dataset (test split)\n",
    "test_dataset = Flowers102(\n",
    "    root='./data',  # The root directory where the dataset will be saved\n",
    "    split='test',   # 'train' for the training set, 'test' for the test set\n",
    "    transform=transform,  # Apply the defined transformation\n",
    "    download=True  # Download if not already present\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Create the optimizer with L2 regularization\n",
    "weight_decay = 1e-5  # Adjust the weight decay hyperparameter\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 287.9732 | Val Loss: 172.3066\n",
      "Train Accuracy: 1.0784% | Val Accuracy: 0.9804%\n",
      "Epoch 2/100 | Train Loss: 231.2639 | Val Loss: 193.8190\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 0.9804%\n",
      "Epoch 3/100 | Train Loss: 229.3506 | Val Loss: 223.1032\n",
      "Train Accuracy: 1.0784% | Val Accuracy: 1.1765%\n",
      "Epoch 4/100 | Train Loss: 240.9500 | Val Loss: 249.5110\n",
      "Train Accuracy: 1.0784% | Val Accuracy: 1.3725%\n",
      "Epoch 5/100 | Train Loss: 229.3734 | Val Loss: 247.6359\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 1.6667%\n",
      "Epoch 6/100 | Train Loss: 238.6291 | Val Loss: 264.4226\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 1.7647%\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 7/100 | Train Loss: 245.6893 | Val Loss: 246.7878\n",
      "Train Accuracy: 1.1765% | Val Accuracy: 1.3725%\n",
      "Epoch 8/100 | Train Loss: 198.5891 | Val Loss: 200.0624\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 1.9608%\n",
      "Epoch 9/100 | Train Loss: 172.8282 | Val Loss: 178.4212\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 1.7647%\n",
      "Epoch 10/100 | Train Loss: 166.0763 | Val Loss: 176.1870\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 1.5686%\n",
      "Epoch 11/100 | Train Loss: 162.2733 | Val Loss: 175.9831\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.3529%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping on validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Create the optimizer with L2 regularization\n",
    "weight_decay = 1e-5  # Adjust the weight decay hyperparameter\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 273.7274 | Val Loss: 167.0861\n",
      "Train Accuracy: 0.7843% | Val Accuracy: 0.9804%\n",
      "Epoch 2/100 | Train Loss: 231.8435 | Val Loss: 199.3567\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 1.1765%\n",
      "Epoch 3/100 | Train Loss: 225.1795 | Val Loss: 210.3886\n",
      "Train Accuracy: 0.6863% | Val Accuracy: 0.9804%\n",
      "Epoch 4/100 | Train Loss: 225.0070 | Val Loss: 242.7624\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 1.2745%\n",
      "Epoch 5/100 | Train Loss: 231.3112 | Val Loss: 246.7956\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 1.4706%\n",
      "Epoch 6/100 | Train Loss: 230.8916 | Val Loss: 245.0402\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 1.7647%\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 7/100 | Train Loss: 240.7831 | Val Loss: 275.0506\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 1.0784%\n",
      "Epoch 8/100 | Train Loss: 213.0923 | Val Loss: 204.1598\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 2.2549%\n",
      "Epoch 9/100 | Train Loss: 176.5596 | Val Loss: 179.3694\n",
      "Train Accuracy: 1.1765% | Val Accuracy: 1.6667%\n",
      "Epoch 10/100 | Train Loss: 165.9017 | Val Loss: 175.7001\n",
      "Train Accuracy: 1.1765% | Val Accuracy: 2.0588%\n",
      "Epoch 11/100 | Train Loss: 161.8770 | Val Loss: 170.3514\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 1.8627%\n",
      "Epoch 12/100 | Train Loss: 160.0601 | Val Loss: 169.1503\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.2549%\n",
      "Epoch 13/100 | Train Loss: 162.2168 | Val Loss: 166.8546\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 1.5686%\n",
      "Epoch 14/100 | Train Loss: 158.7462 | Val Loss: 169.5600\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.0588%\n",
      "Epoch 15/100 | Train Loss: 158.0048 | Val Loss: 167.6787\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 1.5686%\n",
      "Epoch 16/100 | Train Loss: 159.2206 | Val Loss: 164.5953\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 1.2745%\n",
      "Epoch 17/100 | Train Loss: 156.1892 | Val Loss: 165.3261\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 1.8627%\n",
      "Epoch 18/100 | Train Loss: 156.2888 | Val Loss: 165.4437\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.0588%\n",
      "Early stopping. No improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation accuracy.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.5450%\n",
      "Test Loss: 1003.9630\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset (test split)\n",
    "test_dataset = Flowers102(\n",
    "    root='./data',  # The root directory where the dataset will be saved\n",
    "    split='test',   # 'train' for the training set, 'test' for the test set\n",
    "    transform=data_transforms['val'],  # Apply the defined transformation\n",
    "    download=True  # Download if not already present\n",
    ")\n",
    "\n",
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase patience and run more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Create the optimizer with L2 regularization\n",
    "weight_decay = 1e-5  # Adjust the weight decay hyperparameter\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 | Train Loss: 283.2749 | Val Loss: 167.9736\n",
      "Train Accuracy: 0.7843% | Val Accuracy: 0.9804%\n",
      "Epoch 2/1000 | Train Loss: 232.9839 | Val Loss: 194.3428\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 0.6863%\n",
      "Epoch 3/1000 | Train Loss: 227.4693 | Val Loss: 215.7112\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 1.1765%\n",
      "Epoch 4/1000 | Train Loss: 229.1159 | Val Loss: 233.2245\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 1.3725%\n",
      "Epoch 5/1000 | Train Loss: 228.9117 | Val Loss: 259.6232\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 1.2745%\n",
      "Epoch 6/1000 | Train Loss: 238.4372 | Val Loss: 271.4460\n",
      "Train Accuracy: 1.0784% | Val Accuracy: 0.8824%\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 7/1000 | Train Loss: 242.3695 | Val Loss: 258.8447\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 0.9804%\n",
      "Epoch 8/1000 | Train Loss: 207.9759 | Val Loss: 199.4016\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 1.4706%\n",
      "Epoch 9/1000 | Train Loss: 176.5504 | Val Loss: 175.4320\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.2549%\n",
      "Epoch 10/1000 | Train Loss: 162.6852 | Val Loss: 171.9006\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 1.9608%\n",
      "Epoch 11/1000 | Train Loss: 161.8020 | Val Loss: 173.5993\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 2.4510%\n",
      "Epoch 12/1000 | Train Loss: 159.6533 | Val Loss: 168.2080\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 1.8627%\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 13/1000 | Train Loss: 158.9990 | Val Loss: 170.4242\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.4510%\n",
      "Epoch 14/1000 | Train Loss: 155.8358 | Val Loss: 165.6126\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.6471%\n",
      "Epoch 15/1000 | Train Loss: 154.1982 | Val Loss: 166.3612\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.6471%\n",
      "Epoch 16/1000 | Train Loss: 152.5527 | Val Loss: 163.9495\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.7451%\n",
      "Epoch 17/1000 | Train Loss: 151.6504 | Val Loss: 164.3545\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.5490%\n",
      "Epoch 18/1000 | Train Loss: 152.6087 | Val Loss: 161.4905\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.9412%\n",
      "Epoch 19/1000 | Train Loss: 152.6929 | Val Loss: 164.2960\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.5490%\n",
      "Epoch 20/1000 | Train Loss: 152.8982 | Val Loss: 163.8697\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 3.1373%\n",
      "Epoch 21/1000 | Train Loss: 154.2062 | Val Loss: 163.4729\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.8431%\n",
      "Epoch 22/1000 | Train Loss: 152.1413 | Val Loss: 163.2845\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.2353%\n",
      "Epoch 23/1000 | Train Loss: 152.5925 | Val Loss: 162.3712\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.6471%\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 24/1000 | Train Loss: 152.4651 | Val Loss: 161.6574\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.1373%\n",
      "Epoch 25/1000 | Train Loss: 151.7941 | Val Loss: 161.4354\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 3.0392%\n",
      "Epoch 26/1000 | Train Loss: 150.6610 | Val Loss: 162.4228\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.0392%\n",
      "Epoch 27/1000 | Train Loss: 154.1673 | Val Loss: 161.2564\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.7451%\n",
      "Epoch 28/1000 | Train Loss: 152.8354 | Val Loss: 163.9979\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.0392%\n",
      "Epoch 29/1000 | Train Loss: 151.4478 | Val Loss: 161.7300\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.6471%\n",
      "Epoch 30/1000 | Train Loss: 152.0806 | Val Loss: 163.6567\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.4314%\n",
      "Epoch 31/1000 | Train Loss: 151.5054 | Val Loss: 161.2937\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.9412%\n",
      "Epoch 32/1000 | Train Loss: 152.1427 | Val Loss: 164.3465\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 3.1373%\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 33/1000 | Train Loss: 150.0344 | Val Loss: 164.5064\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.7451%\n",
      "Epoch 34/1000 | Train Loss: 153.3742 | Val Loss: 162.6021\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 3.0392%\n",
      "Epoch 35/1000 | Train Loss: 150.7164 | Val Loss: 161.6018\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 2.8431%\n",
      "Epoch 36/1000 | Train Loss: 153.4949 | Val Loss: 160.6150\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 3.2353%\n",
      "Epoch 37/1000 | Train Loss: 152.3200 | Val Loss: 162.3900\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.0392%\n",
      "Epoch 38/1000 | Train Loss: 152.2529 | Val Loss: 163.3763\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.7451%\n",
      "Epoch 39/1000 | Train Loss: 151.3780 | Val Loss: 161.5934\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 3.2353%\n",
      "Epoch 40/1000 | Train Loss: 151.1874 | Val Loss: 164.2967\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 2.6471%\n",
      "Epoch 41/1000 | Train Loss: 152.4984 | Val Loss: 161.2651\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.8431%\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 42/1000 | Train Loss: 150.2730 | Val Loss: 161.2367\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 3.2353%\n",
      "Epoch 43/1000 | Train Loss: 151.3303 | Val Loss: 163.8024\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.0392%\n",
      "Epoch 44/1000 | Train Loss: 153.7169 | Val Loss: 162.9194\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.1373%\n",
      "Epoch 45/1000 | Train Loss: 151.2529 | Val Loss: 160.2921\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 46/1000 | Train Loss: 150.9812 | Val Loss: 163.7820\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.8431%\n",
      "Epoch 47/1000 | Train Loss: 151.2499 | Val Loss: 164.0165\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.7451%\n",
      "Epoch 48/1000 | Train Loss: 151.5441 | Val Loss: 162.4394\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.8431%\n",
      "Epoch 49/1000 | Train Loss: 150.4229 | Val Loss: 165.5216\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 50/1000 | Train Loss: 151.0045 | Val Loss: 161.4243\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 3.1373%\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 51/1000 | Train Loss: 150.8801 | Val Loss: 162.6127\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.9412%\n",
      "Epoch 52/1000 | Train Loss: 150.3136 | Val Loss: 162.4574\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.7451%\n",
      "Epoch 53/1000 | Train Loss: 150.5466 | Val Loss: 161.5143\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.0392%\n",
      "Epoch 54/1000 | Train Loss: 152.5429 | Val Loss: 162.3152\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.5490%\n",
      "Epoch 55/1000 | Train Loss: 151.7401 | Val Loss: 163.5270\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.9412%\n",
      "Epoch 56/1000 | Train Loss: 151.3154 | Val Loss: 162.5759\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.7451%\n",
      "Epoch 57/1000 | Train Loss: 151.7915 | Val Loss: 163.7490\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.8431%\n",
      "Epoch 58/1000 | Train Loss: 150.2812 | Val Loss: 161.0895\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.6471%\n",
      "Epoch 59/1000 | Train Loss: 152.3340 | Val Loss: 162.2207\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.1373%\n",
      "Epoch 60/1000 | Train Loss: 151.4455 | Val Loss: 162.2117\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.8431%\n",
      "Epoch 61/1000 | Train Loss: 151.2112 | Val Loss: 162.4938\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 62/1000 | Train Loss: 150.4652 | Val Loss: 161.2817\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.8431%\n",
      "Epoch 63/1000 | Train Loss: 149.3887 | Val Loss: 160.2950\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.5294%\n",
      "Epoch 64/1000 | Train Loss: 151.2725 | Val Loss: 161.1795\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.2353%\n",
      "Epoch 65/1000 | Train Loss: 154.6461 | Val Loss: 164.2019\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 2.9412%\n",
      "Epoch 66/1000 | Train Loss: 152.6823 | Val Loss: 162.7236\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 3.0392%\n",
      "Epoch 67/1000 | Train Loss: 152.2821 | Val Loss: 162.1491\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.7451%\n",
      "Epoch 68/1000 | Train Loss: 150.6812 | Val Loss: 163.0916\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.9412%\n",
      "Epoch 69/1000 | Train Loss: 151.7828 | Val Loss: 162.4539\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.8431%\n",
      "Epoch 70/1000 | Train Loss: 152.6041 | Val Loss: 161.7677\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.0392%\n",
      "Epoch 71/1000 | Train Loss: 149.4727 | Val Loss: 162.4522\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.0392%\n",
      "Epoch 72/1000 | Train Loss: 149.9804 | Val Loss: 162.4999\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.3333%\n",
      "Epoch 73/1000 | Train Loss: 152.6439 | Val Loss: 163.5205\n",
      "Train Accuracy: 1.1765% | Val Accuracy: 3.0392%\n",
      "Epoch 74/1000 | Train Loss: 151.1683 | Val Loss: 159.5626\n",
      "Train Accuracy: 3.3333% | Val Accuracy: 3.0392%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000 | Train Loss: 152.4730 | Val Loss: 166.1286\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 76/1000 | Train Loss: 151.0049 | Val Loss: 162.8764\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.0392%\n",
      "Epoch 77/1000 | Train Loss: 153.5355 | Val Loss: 162.3327\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.8431%\n",
      "Epoch 78/1000 | Train Loss: 150.9710 | Val Loss: 162.6546\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.8431%\n",
      "Epoch 79/1000 | Train Loss: 150.7171 | Val Loss: 163.2435\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.0392%\n",
      "Epoch 80/1000 | Train Loss: 150.7368 | Val Loss: 162.5604\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.9412%\n",
      "Epoch 81/1000 | Train Loss: 149.9214 | Val Loss: 163.0057\n",
      "Train Accuracy: 3.2353% | Val Accuracy: 2.8431%\n",
      "Epoch 82/1000 | Train Loss: 152.3776 | Val Loss: 161.9532\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.7451%\n",
      "Epoch 83/1000 | Train Loss: 152.7746 | Val Loss: 161.3661\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.9412%\n",
      "Epoch 84/1000 | Train Loss: 151.5944 | Val Loss: 161.5426\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 85/1000 | Train Loss: 149.0645 | Val Loss: 161.9312\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.6471%\n",
      "Epoch 86/1000 | Train Loss: 152.1860 | Val Loss: 163.6478\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.3333%\n",
      "Epoch 87/1000 | Train Loss: 151.5987 | Val Loss: 162.8719\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.9412%\n",
      "Epoch 88/1000 | Train Loss: 152.2477 | Val Loss: 164.0568\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.8431%\n",
      "Epoch 89/1000 | Train Loss: 151.6483 | Val Loss: 162.6116\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.8431%\n",
      "Epoch 90/1000 | Train Loss: 151.0387 | Val Loss: 166.6131\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.2549%\n",
      "Epoch 91/1000 | Train Loss: 150.2291 | Val Loss: 165.4224\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.2549%\n",
      "Epoch 92/1000 | Train Loss: 153.8628 | Val Loss: 163.4005\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.9412%\n",
      "Epoch 93/1000 | Train Loss: 149.1961 | Val Loss: 161.7122\n",
      "Train Accuracy: 3.3333% | Val Accuracy: 3.0392%\n",
      "Epoch 94/1000 | Train Loss: 150.2407 | Val Loss: 162.4295\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.1373%\n",
      "Epoch 95/1000 | Train Loss: 151.5772 | Val Loss: 163.3704\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 96/1000 | Train Loss: 150.2691 | Val Loss: 162.2789\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 3.0392%\n",
      "Epoch 97/1000 | Train Loss: 151.6768 | Val Loss: 163.7788\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 3.0392%\n",
      "Epoch 98/1000 | Train Loss: 152.6569 | Val Loss: 161.5943\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.6471%\n",
      "Epoch 99/1000 | Train Loss: 153.3239 | Val Loss: 161.9084\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.7451%\n",
      "Epoch 100/1000 | Train Loss: 149.6372 | Val Loss: 162.9524\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.7451%\n",
      "Epoch 101/1000 | Train Loss: 152.0832 | Val Loss: 165.1584\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 3.0392%\n",
      "Epoch 102/1000 | Train Loss: 150.3126 | Val Loss: 162.8882\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.5490%\n",
      "Epoch 103/1000 | Train Loss: 148.8001 | Val Loss: 163.1073\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.7451%\n",
      "Epoch 104/1000 | Train Loss: 152.4168 | Val Loss: 163.2376\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 2.6471%\n",
      "Epoch 105/1000 | Train Loss: 150.8525 | Val Loss: 162.9775\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.8431%\n",
      "Epoch 106/1000 | Train Loss: 149.9186 | Val Loss: 162.8851\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.3333%\n",
      "Epoch 107/1000 | Train Loss: 149.9085 | Val Loss: 162.3832\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 3.1373%\n",
      "Epoch 108/1000 | Train Loss: 152.0919 | Val Loss: 162.7105\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.5490%\n",
      "Epoch 109/1000 | Train Loss: 150.5932 | Val Loss: 161.8043\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.6471%\n",
      "Epoch 110/1000 | Train Loss: 152.1086 | Val Loss: 161.5772\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 2.9412%\n",
      "Epoch 111/1000 | Train Loss: 149.6209 | Val Loss: 159.9941\n",
      "Train Accuracy: 3.3333% | Val Accuracy: 2.3529%\n",
      "Epoch 112/1000 | Train Loss: 150.9676 | Val Loss: 159.9614\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.8431%\n",
      "Epoch 113/1000 | Train Loss: 151.4048 | Val Loss: 163.9499\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.5490%\n",
      "Epoch 114/1000 | Train Loss: 151.6044 | Val Loss: 162.1341\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.9412%\n",
      "Epoch 115/1000 | Train Loss: 150.0483 | Val Loss: 164.6865\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.7451%\n",
      "Epoch 116/1000 | Train Loss: 151.7567 | Val Loss: 160.6161\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.0392%\n",
      "Epoch 117/1000 | Train Loss: 149.8462 | Val Loss: 162.4937\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.3333%\n",
      "Epoch 118/1000 | Train Loss: 150.9646 | Val Loss: 163.1821\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.7451%\n",
      "Epoch 119/1000 | Train Loss: 152.1573 | Val Loss: 159.4320\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.8431%\n",
      "Epoch 120/1000 | Train Loss: 149.3410 | Val Loss: 162.4438\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.6471%\n",
      "Epoch 121/1000 | Train Loss: 148.2406 | Val Loss: 162.9624\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.6471%\n",
      "Epoch 122/1000 | Train Loss: 151.7009 | Val Loss: 166.4186\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.9412%\n",
      "Epoch 123/1000 | Train Loss: 151.9310 | Val Loss: 161.3632\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 3.2353%\n",
      "Epoch 124/1000 | Train Loss: 149.8717 | Val Loss: 161.3061\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 3.3333%\n",
      "Epoch 125/1000 | Train Loss: 151.3254 | Val Loss: 165.5336\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.1373%\n",
      "Epoch 126/1000 | Train Loss: 150.8382 | Val Loss: 160.5284\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.6471%\n",
      "Epoch 127/1000 | Train Loss: 151.1581 | Val Loss: 164.5368\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 2.6471%\n",
      "Epoch 128/1000 | Train Loss: 149.9957 | Val Loss: 162.6143\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.8431%\n",
      "Epoch 129/1000 | Train Loss: 151.9974 | Val Loss: 162.9451\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 2.7451%\n",
      "Epoch 130/1000 | Train Loss: 150.9018 | Val Loss: 162.8449\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.0392%\n",
      "Epoch 131/1000 | Train Loss: 151.8355 | Val Loss: 165.9933\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 3.2353%\n",
      "Epoch 132/1000 | Train Loss: 150.5491 | Val Loss: 160.8869\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 3.1373%\n",
      "Epoch 133/1000 | Train Loss: 149.6733 | Val Loss: 162.2334\n",
      "Train Accuracy: 3.3333% | Val Accuracy: 3.0392%\n",
      "Epoch 134/1000 | Train Loss: 152.5171 | Val Loss: 161.0389\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 3.0392%\n",
      "Epoch 135/1000 | Train Loss: 151.7339 | Val Loss: 163.5342\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.8431%\n",
      "Epoch 136/1000 | Train Loss: 150.1897 | Val Loss: 163.3241\n",
      "Train Accuracy: 3.4314% | Val Accuracy: 2.8431%\n",
      "Epoch 137/1000 | Train Loss: 151.1025 | Val Loss: 162.1993\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.9412%\n",
      "Epoch 138/1000 | Train Loss: 151.0618 | Val Loss: 164.3047\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.7451%\n",
      "Epoch 139/1000 | Train Loss: 152.0125 | Val Loss: 162.3003\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.1373%\n",
      "Epoch 140/1000 | Train Loss: 152.0790 | Val Loss: 164.3831\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.0392%\n",
      "Epoch 141/1000 | Train Loss: 151.1145 | Val Loss: 160.9145\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 142/1000 | Train Loss: 151.1199 | Val Loss: 160.4184\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.2353%\n",
      "Epoch 143/1000 | Train Loss: 152.5477 | Val Loss: 161.1986\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.1373%\n",
      "Epoch 144/1000 | Train Loss: 150.6121 | Val Loss: 160.9455\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 2.4510%\n",
      "Epoch 145/1000 | Train Loss: 150.0335 | Val Loss: 165.1134\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.8431%\n",
      "Epoch 146/1000 | Train Loss: 149.4178 | Val Loss: 162.5277\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 3.2353%\n",
      "Epoch 147/1000 | Train Loss: 151.1707 | Val Loss: 162.7787\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.8431%\n",
      "Epoch 148/1000 | Train Loss: 150.9017 | Val Loss: 164.3146\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.7451%\n",
      "Epoch 149/1000 | Train Loss: 150.4405 | Val Loss: 161.2303\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.8431%\n",
      "Epoch 150/1000 | Train Loss: 153.5374 | Val Loss: 162.4205\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.4510%\n",
      "Epoch 151/1000 | Train Loss: 150.8813 | Val Loss: 161.0469\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.6471%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/1000 | Train Loss: 152.3743 | Val Loss: 163.6686\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.6471%\n",
      "Epoch 153/1000 | Train Loss: 152.3550 | Val Loss: 165.6207\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 154/1000 | Train Loss: 151.6111 | Val Loss: 161.6908\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.3333%\n",
      "Epoch 155/1000 | Train Loss: 152.5840 | Val Loss: 160.5691\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.8431%\n",
      "Epoch 156/1000 | Train Loss: 149.4678 | Val Loss: 164.2280\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.1569%\n",
      "Epoch 157/1000 | Train Loss: 151.2140 | Val Loss: 163.6436\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.8431%\n",
      "Epoch 158/1000 | Train Loss: 151.8550 | Val Loss: 163.9117\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.9412%\n",
      "Epoch 159/1000 | Train Loss: 149.4272 | Val Loss: 163.8863\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.9412%\n",
      "Epoch 160/1000 | Train Loss: 152.6269 | Val Loss: 161.5398\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.9412%\n",
      "Epoch 161/1000 | Train Loss: 150.1608 | Val Loss: 160.5251\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.9412%\n",
      "Epoch 162/1000 | Train Loss: 149.6216 | Val Loss: 161.7979\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.8431%\n",
      "Epoch 163/1000 | Train Loss: 151.3540 | Val Loss: 162.3608\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.8431%\n",
      "Epoch 164/1000 | Train Loss: 150.8272 | Val Loss: 162.6516\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 3.1373%\n",
      "Epoch 165/1000 | Train Loss: 150.9217 | Val Loss: 162.5018\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 166/1000 | Train Loss: 151.1804 | Val Loss: 161.5377\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 3.1373%\n",
      "Epoch 167/1000 | Train Loss: 150.8941 | Val Loss: 161.6787\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 3.3333%\n",
      "Epoch 168/1000 | Train Loss: 150.6699 | Val Loss: 162.5583\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.8431%\n",
      "Epoch 169/1000 | Train Loss: 150.9325 | Val Loss: 162.2436\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.3529%\n",
      "Epoch 170/1000 | Train Loss: 152.1080 | Val Loss: 163.5920\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.5490%\n",
      "Epoch 171/1000 | Train Loss: 151.8872 | Val Loss: 161.9670\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.3333%\n",
      "Epoch 172/1000 | Train Loss: 150.9205 | Val Loss: 162.8762\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 173/1000 | Train Loss: 150.8504 | Val Loss: 163.3353\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.7451%\n",
      "Epoch 174/1000 | Train Loss: 151.3280 | Val Loss: 162.9474\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.8431%\n",
      "Epoch 175/1000 | Train Loss: 151.9159 | Val Loss: 161.6211\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 176/1000 | Train Loss: 152.9068 | Val Loss: 161.7119\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 3.0392%\n",
      "Epoch 177/1000 | Train Loss: 151.7992 | Val Loss: 163.5065\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.6471%\n",
      "Epoch 178/1000 | Train Loss: 152.0851 | Val Loss: 164.0840\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.8431%\n",
      "Epoch 179/1000 | Train Loss: 152.0757 | Val Loss: 163.2306\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.2353%\n",
      "Epoch 180/1000 | Train Loss: 151.0669 | Val Loss: 163.6371\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.8431%\n",
      "Epoch 181/1000 | Train Loss: 149.9548 | Val Loss: 162.5766\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.5490%\n",
      "Epoch 182/1000 | Train Loss: 150.0819 | Val Loss: 162.3091\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.6471%\n",
      "Epoch 183/1000 | Train Loss: 152.0598 | Val Loss: 162.4842\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.0392%\n",
      "Epoch 184/1000 | Train Loss: 153.6989 | Val Loss: 160.0120\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 3.0392%\n",
      "Epoch 185/1000 | Train Loss: 153.5597 | Val Loss: 165.8917\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.4510%\n",
      "Epoch 186/1000 | Train Loss: 149.7371 | Val Loss: 162.3834\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 187/1000 | Train Loss: 151.6784 | Val Loss: 162.8367\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.8431%\n",
      "Epoch 188/1000 | Train Loss: 150.9501 | Val Loss: 163.4271\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 189/1000 | Train Loss: 151.8722 | Val Loss: 162.1727\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.7451%\n",
      "Epoch 190/1000 | Train Loss: 151.9524 | Val Loss: 164.5189\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.5490%\n",
      "Epoch 191/1000 | Train Loss: 150.4766 | Val Loss: 160.6892\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.9412%\n",
      "Epoch 192/1000 | Train Loss: 152.1774 | Val Loss: 161.0071\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 193/1000 | Train Loss: 151.6297 | Val Loss: 164.8543\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.8431%\n",
      "Epoch 194/1000 | Train Loss: 151.5601 | Val Loss: 162.2200\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.7451%\n",
      "Epoch 195/1000 | Train Loss: 149.5191 | Val Loss: 159.2500\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.2549%\n",
      "Epoch 196/1000 | Train Loss: 151.0942 | Val Loss: 161.5661\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.0392%\n",
      "Epoch 197/1000 | Train Loss: 151.5944 | Val Loss: 162.1185\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.7451%\n",
      "Epoch 198/1000 | Train Loss: 150.9236 | Val Loss: 162.6313\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 3.1373%\n",
      "Epoch 199/1000 | Train Loss: 152.0763 | Val Loss: 163.6037\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 200/1000 | Train Loss: 151.5646 | Val Loss: 163.9764\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.9412%\n",
      "Epoch 201/1000 | Train Loss: 151.5113 | Val Loss: 162.7351\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.9412%\n",
      "Epoch 202/1000 | Train Loss: 151.7207 | Val Loss: 160.6952\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.7451%\n",
      "Epoch 203/1000 | Train Loss: 150.5055 | Val Loss: 161.6377\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.3333%\n",
      "Epoch 204/1000 | Train Loss: 152.3932 | Val Loss: 163.7465\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.0392%\n",
      "Epoch 205/1000 | Train Loss: 151.0072 | Val Loss: 160.2105\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.0392%\n",
      "Epoch 206/1000 | Train Loss: 151.4706 | Val Loss: 162.3693\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 3.3333%\n",
      "Epoch 207/1000 | Train Loss: 150.8613 | Val Loss: 161.5712\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.4510%\n",
      "Epoch 208/1000 | Train Loss: 151.0908 | Val Loss: 161.5890\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 209/1000 | Train Loss: 151.7229 | Val Loss: 160.5763\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 3.6275%\n",
      "Epoch 210/1000 | Train Loss: 150.9982 | Val Loss: 163.3751\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.7451%\n",
      "Epoch 211/1000 | Train Loss: 150.7043 | Val Loss: 163.7775\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 212/1000 | Train Loss: 150.9317 | Val Loss: 162.6810\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.0392%\n",
      "Epoch 213/1000 | Train Loss: 151.1729 | Val Loss: 165.2822\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 214/1000 | Train Loss: 150.6343 | Val Loss: 163.9630\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.2353%\n",
      "Epoch 215/1000 | Train Loss: 152.0618 | Val Loss: 162.1184\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 3.0392%\n",
      "Epoch 216/1000 | Train Loss: 151.2900 | Val Loss: 162.0415\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 217/1000 | Train Loss: 151.2764 | Val Loss: 163.3202\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.8431%\n",
      "Epoch 218/1000 | Train Loss: 151.6777 | Val Loss: 162.4421\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.6471%\n",
      "Epoch 219/1000 | Train Loss: 151.7201 | Val Loss: 162.0561\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.6471%\n",
      "Epoch 220/1000 | Train Loss: 151.7152 | Val Loss: 161.6267\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.0392%\n",
      "Epoch 221/1000 | Train Loss: 152.7476 | Val Loss: 164.4626\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.9412%\n",
      "Epoch 222/1000 | Train Loss: 152.3943 | Val Loss: 165.0478\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 223/1000 | Train Loss: 152.6682 | Val Loss: 163.0097\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.7451%\n",
      "Epoch 224/1000 | Train Loss: 150.2739 | Val Loss: 161.0969\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.3333%\n",
      "Epoch 225/1000 | Train Loss: 151.0925 | Val Loss: 161.8262\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 226/1000 | Train Loss: 151.9958 | Val Loss: 163.2803\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.0392%\n",
      "Epoch 227/1000 | Train Loss: 152.2938 | Val Loss: 165.4157\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.5490%\n",
      "Epoch 228/1000 | Train Loss: 152.2556 | Val Loss: 163.9243\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.8431%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000 | Train Loss: 150.6114 | Val Loss: 161.4575\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 3.2353%\n",
      "Epoch 230/1000 | Train Loss: 153.0568 | Val Loss: 164.2310\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 3.2353%\n",
      "Epoch 231/1000 | Train Loss: 151.7937 | Val Loss: 162.7951\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 3.2353%\n",
      "Epoch 232/1000 | Train Loss: 149.9890 | Val Loss: 164.2544\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 2.8431%\n",
      "Epoch 233/1000 | Train Loss: 150.3886 | Val Loss: 160.5576\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.7451%\n",
      "Epoch 234/1000 | Train Loss: 151.0927 | Val Loss: 162.0423\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.2353%\n",
      "Epoch 235/1000 | Train Loss: 150.2363 | Val Loss: 159.4278\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.9412%\n",
      "Epoch 236/1000 | Train Loss: 151.7109 | Val Loss: 162.2822\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 2.9412%\n",
      "Epoch 237/1000 | Train Loss: 151.7644 | Val Loss: 161.8038\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.8431%\n",
      "Epoch 238/1000 | Train Loss: 152.4312 | Val Loss: 161.0182\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.7451%\n",
      "Epoch 239/1000 | Train Loss: 150.6637 | Val Loss: 167.4469\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 3.0392%\n",
      "Epoch 240/1000 | Train Loss: 148.1498 | Val Loss: 163.9052\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.9412%\n",
      "Epoch 241/1000 | Train Loss: 150.7101 | Val Loss: 163.1851\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.2549%\n",
      "Epoch 242/1000 | Train Loss: 151.4733 | Val Loss: 165.0820\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.7451%\n",
      "Epoch 243/1000 | Train Loss: 150.7390 | Val Loss: 163.9016\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 3.2353%\n",
      "Epoch 244/1000 | Train Loss: 150.2231 | Val Loss: 165.7763\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.5490%\n",
      "Epoch 245/1000 | Train Loss: 152.6001 | Val Loss: 162.8667\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.1373%\n",
      "Epoch 246/1000 | Train Loss: 151.5358 | Val Loss: 163.9779\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.8431%\n",
      "Epoch 247/1000 | Train Loss: 149.4574 | Val Loss: 163.6180\n",
      "Train Accuracy: 3.3333% | Val Accuracy: 2.6471%\n",
      "Epoch 248/1000 | Train Loss: 151.4525 | Val Loss: 164.5001\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.1373%\n",
      "Epoch 249/1000 | Train Loss: 151.0619 | Val Loss: 159.8375\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.6471%\n",
      "Epoch 250/1000 | Train Loss: 150.1026 | Val Loss: 161.3126\n",
      "Train Accuracy: 3.5294% | Val Accuracy: 2.5490%\n",
      "Epoch 251/1000 | Train Loss: 151.1287 | Val Loss: 161.5389\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 252/1000 | Train Loss: 151.8300 | Val Loss: 162.4797\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.8431%\n",
      "Epoch 253/1000 | Train Loss: 151.1380 | Val Loss: 160.1998\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.6471%\n",
      "Epoch 254/1000 | Train Loss: 151.6152 | Val Loss: 163.4339\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.8431%\n",
      "Epoch 255/1000 | Train Loss: 150.6328 | Val Loss: 160.8812\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 256/1000 | Train Loss: 151.1547 | Val Loss: 162.9129\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.8431%\n",
      "Epoch 257/1000 | Train Loss: 151.3266 | Val Loss: 163.3622\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.7451%\n",
      "Epoch 258/1000 | Train Loss: 151.0564 | Val Loss: 164.5076\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 3.0392%\n",
      "Epoch 259/1000 | Train Loss: 151.0598 | Val Loss: 162.4983\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.9412%\n",
      "Epoch 260/1000 | Train Loss: 150.7112 | Val Loss: 162.6727\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 3.1373%\n",
      "Epoch 261/1000 | Train Loss: 150.1192 | Val Loss: 163.8882\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 262/1000 | Train Loss: 150.6704 | Val Loss: 164.5855\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.1373%\n",
      "Epoch 263/1000 | Train Loss: 151.4181 | Val Loss: 162.2998\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.8431%\n",
      "Epoch 264/1000 | Train Loss: 150.1902 | Val Loss: 162.4927\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.7451%\n",
      "Epoch 265/1000 | Train Loss: 148.8388 | Val Loss: 165.6000\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 2.9412%\n",
      "Epoch 266/1000 | Train Loss: 149.1447 | Val Loss: 160.8806\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 2.9412%\n",
      "Epoch 267/1000 | Train Loss: 149.9932 | Val Loss: 161.4062\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.8431%\n",
      "Epoch 268/1000 | Train Loss: 150.8368 | Val Loss: 164.1429\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.6471%\n",
      "Epoch 269/1000 | Train Loss: 151.8729 | Val Loss: 164.2579\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 3.2353%\n",
      "Epoch 270/1000 | Train Loss: 153.0413 | Val Loss: 162.7538\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.8431%\n",
      "Epoch 271/1000 | Train Loss: 151.9901 | Val Loss: 162.7046\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.4510%\n",
      "Epoch 272/1000 | Train Loss: 151.5483 | Val Loss: 163.3124\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 3.1373%\n",
      "Epoch 273/1000 | Train Loss: 151.1451 | Val Loss: 161.5370\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.8431%\n",
      "Epoch 274/1000 | Train Loss: 151.1449 | Val Loss: 161.4533\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.9412%\n",
      "Epoch 275/1000 | Train Loss: 151.2206 | Val Loss: 161.6222\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.9412%\n",
      "Epoch 276/1000 | Train Loss: 150.5964 | Val Loss: 164.6565\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.3529%\n",
      "Epoch 277/1000 | Train Loss: 151.2605 | Val Loss: 159.4670\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.1373%\n",
      "Epoch 278/1000 | Train Loss: 152.0288 | Val Loss: 166.0275\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.7451%\n",
      "Epoch 279/1000 | Train Loss: 150.7036 | Val Loss: 163.7434\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 3.0392%\n",
      "Epoch 280/1000 | Train Loss: 150.0241 | Val Loss: 162.8700\n",
      "Train Accuracy: 3.4314% | Val Accuracy: 2.5490%\n",
      "Epoch 281/1000 | Train Loss: 150.2639 | Val Loss: 161.7496\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.1373%\n",
      "Epoch 282/1000 | Train Loss: 152.0503 | Val Loss: 161.6779\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.6471%\n",
      "Epoch 283/1000 | Train Loss: 152.1816 | Val Loss: 163.3735\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.8431%\n",
      "Epoch 284/1000 | Train Loss: 151.3572 | Val Loss: 164.3559\n",
      "Train Accuracy: 3.4314% | Val Accuracy: 2.8431%\n",
      "Epoch 285/1000 | Train Loss: 151.5435 | Val Loss: 161.1671\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 286/1000 | Train Loss: 152.4429 | Val Loss: 162.1941\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.3333%\n",
      "Epoch 287/1000 | Train Loss: 151.6631 | Val Loss: 161.6673\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.8431%\n",
      "Epoch 288/1000 | Train Loss: 151.7663 | Val Loss: 164.3851\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.7451%\n",
      "Epoch 289/1000 | Train Loss: 152.1828 | Val Loss: 167.6915\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.2549%\n",
      "Epoch 290/1000 | Train Loss: 151.6107 | Val Loss: 163.4817\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.9412%\n",
      "Epoch 291/1000 | Train Loss: 153.0693 | Val Loss: 161.9660\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.7451%\n",
      "Epoch 292/1000 | Train Loss: 151.6323 | Val Loss: 161.2099\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 3.0392%\n",
      "Epoch 293/1000 | Train Loss: 150.9615 | Val Loss: 166.3725\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.4510%\n",
      "Epoch 294/1000 | Train Loss: 151.1737 | Val Loss: 161.3712\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 295/1000 | Train Loss: 151.3673 | Val Loss: 163.0478\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 2.7451%\n",
      "Epoch 296/1000 | Train Loss: 152.1759 | Val Loss: 162.2022\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.9412%\n",
      "Epoch 297/1000 | Train Loss: 151.0762 | Val Loss: 162.3416\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 298/1000 | Train Loss: 151.3609 | Val Loss: 162.3692\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.9412%\n",
      "Epoch 299/1000 | Train Loss: 148.9467 | Val Loss: 161.7271\n",
      "Train Accuracy: 2.9412% | Val Accuracy: 2.9412%\n",
      "Epoch 300/1000 | Train Loss: 151.7015 | Val Loss: 163.1335\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.8431%\n",
      "Epoch 301/1000 | Train Loss: 150.7732 | Val Loss: 165.1456\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.3333%\n",
      "Epoch 302/1000 | Train Loss: 153.5806 | Val Loss: 161.8523\n",
      "Train Accuracy: 1.1765% | Val Accuracy: 3.1373%\n",
      "Epoch 303/1000 | Train Loss: 151.3226 | Val Loss: 162.5936\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.6471%\n",
      "Epoch 304/1000 | Train Loss: 151.2016 | Val Loss: 163.1784\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.2353%\n",
      "Epoch 305/1000 | Train Loss: 152.9627 | Val Loss: 163.5834\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.9412%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/1000 | Train Loss: 152.5378 | Val Loss: 163.1750\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.7451%\n",
      "Epoch 307/1000 | Train Loss: 152.5363 | Val Loss: 163.8353\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 3.2353%\n",
      "Epoch 308/1000 | Train Loss: 151.7873 | Val Loss: 160.5030\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.1373%\n",
      "Epoch 309/1000 | Train Loss: 150.1780 | Val Loss: 163.6077\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.6471%\n",
      "Epoch 310/1000 | Train Loss: 150.6227 | Val Loss: 161.7239\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.4510%\n",
      "Epoch 311/1000 | Train Loss: 150.5081 | Val Loss: 160.9311\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 3.2353%\n",
      "Epoch 312/1000 | Train Loss: 150.3413 | Val Loss: 163.0417\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.1373%\n",
      "Epoch 313/1000 | Train Loss: 152.4786 | Val Loss: 160.7105\n",
      "Train Accuracy: 3.1373% | Val Accuracy: 3.0392%\n",
      "Epoch 314/1000 | Train Loss: 151.9402 | Val Loss: 165.2271\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.7451%\n",
      "Epoch 315/1000 | Train Loss: 150.6660 | Val Loss: 161.3554\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.1373%\n",
      "Epoch 316/1000 | Train Loss: 153.2260 | Val Loss: 160.5722\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.7451%\n",
      "Epoch 317/1000 | Train Loss: 152.4795 | Val Loss: 160.7373\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.2549%\n",
      "Epoch 318/1000 | Train Loss: 150.9121 | Val Loss: 164.0022\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.1373%\n",
      "Epoch 319/1000 | Train Loss: 149.1175 | Val Loss: 163.7370\n",
      "Train Accuracy: 1.6667% | Val Accuracy: 2.8431%\n",
      "Epoch 320/1000 | Train Loss: 151.9924 | Val Loss: 165.9063\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 321/1000 | Train Loss: 150.7258 | Val Loss: 167.0887\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 2.8431%\n",
      "Epoch 322/1000 | Train Loss: 149.2929 | Val Loss: 161.8413\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 2.5490%\n",
      "Epoch 323/1000 | Train Loss: 152.1366 | Val Loss: 164.1192\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 3.2353%\n",
      "Epoch 324/1000 | Train Loss: 150.4648 | Val Loss: 161.5032\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.0392%\n",
      "Epoch 325/1000 | Train Loss: 152.2840 | Val Loss: 161.6934\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.8431%\n",
      "Epoch 326/1000 | Train Loss: 151.3333 | Val Loss: 162.7569\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.8431%\n",
      "Epoch 327/1000 | Train Loss: 151.3476 | Val Loss: 160.5097\n",
      "Train Accuracy: 3.3333% | Val Accuracy: 3.0392%\n",
      "Epoch 328/1000 | Train Loss: 151.3495 | Val Loss: 160.7864\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.0392%\n",
      "Epoch 329/1000 | Train Loss: 151.9669 | Val Loss: 164.8702\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 3.0392%\n",
      "Epoch 330/1000 | Train Loss: 150.5847 | Val Loss: 165.8829\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.6471%\n",
      "Epoch 331/1000 | Train Loss: 150.2439 | Val Loss: 162.1530\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 2.8431%\n",
      "Epoch 332/1000 | Train Loss: 152.8104 | Val Loss: 166.1609\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.4314%\n",
      "Epoch 333/1000 | Train Loss: 150.5497 | Val Loss: 163.2282\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 3.1373%\n",
      "Epoch 334/1000 | Train Loss: 151.4302 | Val Loss: 159.9067\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.9412%\n",
      "Epoch 335/1000 | Train Loss: 151.5828 | Val Loss: 162.9838\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.6471%\n",
      "Epoch 336/1000 | Train Loss: 150.5499 | Val Loss: 162.9402\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.9412%\n",
      "Epoch 337/1000 | Train Loss: 150.5927 | Val Loss: 164.1472\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.1373%\n",
      "Epoch 338/1000 | Train Loss: 150.1512 | Val Loss: 162.4460\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 3.4314%\n",
      "Epoch 339/1000 | Train Loss: 150.8906 | Val Loss: 162.4943\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.5490%\n",
      "Epoch 340/1000 | Train Loss: 150.3229 | Val Loss: 161.9169\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.4510%\n",
      "Epoch 341/1000 | Train Loss: 151.9249 | Val Loss: 163.0484\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.7451%\n",
      "Epoch 342/1000 | Train Loss: 150.4263 | Val Loss: 160.8389\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.9412%\n",
      "Epoch 343/1000 | Train Loss: 150.9692 | Val Loss: 162.0855\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 344/1000 | Train Loss: 151.2358 | Val Loss: 163.0223\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 3.0392%\n",
      "Epoch 345/1000 | Train Loss: 151.2525 | Val Loss: 163.3122\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.5490%\n",
      "Epoch 346/1000 | Train Loss: 149.7247 | Val Loss: 161.5922\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 347/1000 | Train Loss: 150.2398 | Val Loss: 162.8476\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.9412%\n",
      "Epoch 348/1000 | Train Loss: 151.7964 | Val Loss: 161.7606\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.4510%\n",
      "Epoch 349/1000 | Train Loss: 150.2266 | Val Loss: 162.6044\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.9412%\n",
      "Epoch 350/1000 | Train Loss: 152.3147 | Val Loss: 165.7881\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.4510%\n",
      "Epoch 351/1000 | Train Loss: 150.2309 | Val Loss: 163.2177\n",
      "Train Accuracy: 3.7255% | Val Accuracy: 2.7451%\n",
      "Epoch 352/1000 | Train Loss: 152.7464 | Val Loss: 161.7081\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.9412%\n",
      "Epoch 353/1000 | Train Loss: 150.6065 | Val Loss: 163.4892\n",
      "Train Accuracy: 1.3725% | Val Accuracy: 2.8431%\n",
      "Epoch 354/1000 | Train Loss: 150.9403 | Val Loss: 162.6053\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.8431%\n",
      "Epoch 355/1000 | Train Loss: 154.6650 | Val Loss: 165.4086\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.1373%\n",
      "Epoch 356/1000 | Train Loss: 151.0159 | Val Loss: 163.0904\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.6471%\n",
      "Epoch 357/1000 | Train Loss: 149.3793 | Val Loss: 161.8459\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.9412%\n",
      "Epoch 358/1000 | Train Loss: 152.2894 | Val Loss: 163.2202\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.0392%\n",
      "Epoch 359/1000 | Train Loss: 150.2451 | Val Loss: 163.0569\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.8431%\n",
      "Epoch 360/1000 | Train Loss: 149.2012 | Val Loss: 164.5579\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.8431%\n",
      "Epoch 361/1000 | Train Loss: 151.4066 | Val Loss: 161.8212\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 2.9412%\n",
      "Epoch 362/1000 | Train Loss: 151.5151 | Val Loss: 162.6423\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.8431%\n",
      "Epoch 363/1000 | Train Loss: 152.3818 | Val Loss: 164.0727\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 2.6471%\n",
      "Epoch 364/1000 | Train Loss: 151.0845 | Val Loss: 164.3898\n",
      "Train Accuracy: 3.2353% | Val Accuracy: 2.5490%\n",
      "Epoch 365/1000 | Train Loss: 149.4987 | Val Loss: 162.0150\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 3.3333%\n",
      "Epoch 366/1000 | Train Loss: 149.0782 | Val Loss: 161.7014\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.1373%\n",
      "Epoch 367/1000 | Train Loss: 151.3973 | Val Loss: 159.8428\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.8431%\n",
      "Epoch 368/1000 | Train Loss: 151.4016 | Val Loss: 163.3363\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 2.7451%\n",
      "Epoch 369/1000 | Train Loss: 152.3560 | Val Loss: 162.3485\n",
      "Train Accuracy: 1.4706% | Val Accuracy: 2.9412%\n",
      "Epoch 370/1000 | Train Loss: 151.4070 | Val Loss: 163.7823\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.3333%\n",
      "Epoch 371/1000 | Train Loss: 152.5643 | Val Loss: 162.7605\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.4510%\n",
      "Epoch 372/1000 | Train Loss: 151.9431 | Val Loss: 163.6645\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.8431%\n",
      "Epoch 373/1000 | Train Loss: 150.8561 | Val Loss: 163.3690\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.3529%\n",
      "Epoch 374/1000 | Train Loss: 150.2608 | Val Loss: 163.1233\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.0392%\n",
      "Epoch 375/1000 | Train Loss: 152.6320 | Val Loss: 160.5308\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 2.9412%\n",
      "Epoch 376/1000 | Train Loss: 152.3949 | Val Loss: 161.4001\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.9412%\n",
      "Epoch 377/1000 | Train Loss: 149.6827 | Val Loss: 160.5959\n",
      "Train Accuracy: 3.3333% | Val Accuracy: 3.0392%\n",
      "Epoch 378/1000 | Train Loss: 151.8019 | Val Loss: 164.4815\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.2353%\n",
      "Epoch 379/1000 | Train Loss: 150.7383 | Val Loss: 161.8649\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.3333%\n",
      "Epoch 380/1000 | Train Loss: 151.4564 | Val Loss: 161.8434\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 3.0392%\n",
      "Epoch 381/1000 | Train Loss: 152.7999 | Val Loss: 164.3630\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.1569%\n",
      "Epoch 382/1000 | Train Loss: 151.8826 | Val Loss: 165.6606\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 3.0392%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/1000 | Train Loss: 152.2491 | Val Loss: 163.0207\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.7451%\n",
      "Epoch 384/1000 | Train Loss: 153.3365 | Val Loss: 164.3520\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 2.8431%\n",
      "Epoch 385/1000 | Train Loss: 151.9792 | Val Loss: 163.3525\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.7451%\n",
      "Epoch 386/1000 | Train Loss: 152.9864 | Val Loss: 161.7207\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 2.6471%\n",
      "Epoch 387/1000 | Train Loss: 152.8803 | Val Loss: 163.0214\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 3.4314%\n",
      "Epoch 388/1000 | Train Loss: 151.9118 | Val Loss: 161.9342\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.8431%\n",
      "Epoch 389/1000 | Train Loss: 149.3963 | Val Loss: 162.0129\n",
      "Train Accuracy: 3.5294% | Val Accuracy: 2.9412%\n",
      "Epoch 390/1000 | Train Loss: 152.2006 | Val Loss: 161.3556\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 3.0392%\n",
      "Epoch 391/1000 | Train Loss: 150.1553 | Val Loss: 161.9082\n",
      "Train Accuracy: 2.2549% | Val Accuracy: 3.0392%\n",
      "Epoch 392/1000 | Train Loss: 153.0266 | Val Loss: 164.0968\n",
      "Train Accuracy: 1.9608% | Val Accuracy: 3.3333%\n",
      "Epoch 393/1000 | Train Loss: 149.9694 | Val Loss: 161.8219\n",
      "Train Accuracy: 2.6471% | Val Accuracy: 2.8431%\n",
      "Epoch 394/1000 | Train Loss: 151.4040 | Val Loss: 162.3666\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.5490%\n",
      "Epoch 395/1000 | Train Loss: 151.3134 | Val Loss: 161.2504\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.8431%\n",
      "Epoch 396/1000 | Train Loss: 152.2110 | Val Loss: 163.1154\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.3333%\n",
      "Epoch 397/1000 | Train Loss: 151.2587 | Val Loss: 161.4553\n",
      "Train Accuracy: 1.8627% | Val Accuracy: 2.7451%\n",
      "Epoch 398/1000 | Train Loss: 151.0756 | Val Loss: 163.3401\n",
      "Train Accuracy: 2.5490% | Val Accuracy: 2.7451%\n",
      "Epoch 399/1000 | Train Loss: 149.3067 | Val Loss: 162.0018\n",
      "Train Accuracy: 3.0392% | Val Accuracy: 2.7451%\n",
      "Epoch 400/1000 | Train Loss: 151.1811 | Val Loss: 161.4025\n",
      "Train Accuracy: 2.4510% | Val Accuracy: 3.0392%\n",
      "Epoch 401/1000 | Train Loss: 149.2200 | Val Loss: 162.8781\n",
      "Train Accuracy: 3.7255% | Val Accuracy: 2.7451%\n",
      "Epoch 402/1000 | Train Loss: 150.9122 | Val Loss: 161.9648\n",
      "Train Accuracy: 2.8431% | Val Accuracy: 3.0392%\n",
      "Epoch 403/1000 | Train Loss: 149.6863 | Val Loss: 163.1900\n",
      "Train Accuracy: 2.3529% | Val Accuracy: 2.9412%\n",
      "Epoch 404/1000 | Train Loss: 151.5137 | Val Loss: 160.4680\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 2.8431%\n",
      "Epoch 405/1000 | Train Loss: 150.7026 | Val Loss: 163.3965\n",
      "Train Accuracy: 2.7451% | Val Accuracy: 3.2353%\n",
      "Epoch 406/1000 | Train Loss: 150.9321 | Val Loss: 162.0051\n",
      "Train Accuracy: 1.7647% | Val Accuracy: 3.0392%\n",
      "Epoch 407/1000 | Train Loss: 151.5820 | Val Loss: 161.6870\n",
      "Train Accuracy: 2.1569% | Val Accuracy: 2.7451%\n",
      "Epoch 408/1000 | Train Loss: 150.6365 | Val Loss: 165.0995\n",
      "Train Accuracy: 2.0588% | Val Accuracy: 2.8431%\n",
      "Epoch 409/1000 | Train Loss: 152.2268 | Val Loss: 162.6307\n",
      "Train Accuracy: 1.5686% | Val Accuracy: 3.4314%\n",
      "Early stopping. No improvement in validation accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 1000\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0\n",
    "patience = 200  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation accuracy.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.8214%\n",
      "Test Loss: 975.5179\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing of image to 500x500 -> edit first layer of resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser/anaconda3/envs/flowers2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Flowers102\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define data transformations for data augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((500, 500)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),  # You can adjust the rotation angle\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((500, 500)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create data loaders with data augmentation\n",
    "train_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    transform=data_transforms['train'],  # Apply data augmentation to the training set\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='val',\n",
    "    transform=data_transforms['val'],  # Use the validation data transformation\n",
    "    download=True\n",
    ")\n",
    "\n",
    "\n",
    "# Download the dataset (test split)\n",
    "test_dataset = Flowers102(\n",
    "    root='./data',  # The root directory where the dataset will be saved\n",
    "    split='test',   # 'train' for the training set, 'test' for the test set\n",
    "    transform=data_transforms['val'],  # Apply the defined transformation\n",
    "    download=True  # Download if not already present\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the first convolutional layer to accept 3-channel (RGB) images of size 500x500\n",
    "# You need to change the in_channels argument to 3 and kernel_size to 7\n",
    "resnet50.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Freeze the parameters of the modified layers\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Remove the classification head (fully connected layers)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "def generate_attention_masks(images, model):\n",
    "    model.eval()\n",
    "    \n",
    "    images = images.to(device)  # Move inputs to the same device as the model (e.g., GPU)\n",
    "    model = model.to(device)  # Move the model to the same device as inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    attention_masks = []\n",
    "    for prediction in predictions:\n",
    "        # Extract the region proposals and their scores for each image in the batch\n",
    "        proposals = prediction['boxes']\n",
    "        scores = prediction['scores']\n",
    "\n",
    "        # You can adjust this threshold to select ROIs based on their confidence score\n",
    "        threshold = 0.3\n",
    "        selected_indices = scores > threshold\n",
    "\n",
    "        # Convert selected_indices to integers\n",
    "        selected_indices = selected_indices.nonzero(as_tuple=False).squeeze(dim=1).long()\n",
    "\n",
    "        # Create an attention mask for each image in the batch\n",
    "        attention_mask = torch.zeros_like(images[0, 0, :, :])  # Assuming images is a batch of shape (N, C, H, W)\n",
    "        attention_mask[proposals[selected_indices, 1].long(), proposals[selected_indices, 0].long()] = 1.0  # Explicitly cast to long\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "# You can choose the architecture you prefer and adjust it accordingly\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rcnn.eval()\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, resnet, rcnn, num_classes, device):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rcnn = rcnn\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Assuming 2048 is the output feature size of the ResNet\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention masks using the Faster R-CNN\n",
    "        attention_masks = generate_attention_masks(x, self.rcnn)\n",
    "        # Concatenate the list of attention masks into a single tensor\n",
    "        attention_mask = torch.stack(attention_masks, dim=0).to(self.device)  # Move to the desired device\n",
    "        x = x.to(self.device)  # Move the input data to the same device\n",
    "        # Apply the attention mask to the input images\n",
    "        x = x * attention_mask.unsqueeze(1)\n",
    "        # Pass the modified image through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "num_classes = 102  # Number of classes in your dataset\n",
    "combined_model = ResNetWithAttention(resnet50, model_rcnn, num_classes, device)\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Create the optimizer with L2 regularization\n",
    "weight_decay = 1e-5  # Adjust the weight decay hyperparameter\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 221.4124 | Val Loss: 152.7389\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 0.9804%\n",
      "Epoch 2/10 | Train Loss: 194.3204 | Val Loss: 166.7170\n",
      "Train Accuracy: 0.5882% | Val Accuracy: 0.9804%\n",
      "Epoch 3/10 | Train Loss: 187.5235 | Val Loss: 172.6962\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 1.1765%\n",
      "Epoch 4/10 | Train Loss: 190.1003 | Val Loss: 188.9038\n",
      "Train Accuracy: 0.8824% | Val Accuracy: 1.2745%\n",
      "Epoch 5/10 | Train Loss: 189.0442 | Val Loss: 199.4049\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 1.3725%\n",
      "Epoch 6/10 | Train Loss: 191.5788 | Val Loss: 195.9536\n",
      "Train Accuracy: 0.7843% | Val Accuracy: 1.7647%\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 7/10 | Train Loss: 187.8411 | Val Loss: 190.4818\n",
      "Train Accuracy: 1.2745% | Val Accuracy: 1.0784%\n",
      "Epoch 8/10 | Train Loss: 170.3034 | Val Loss: 163.3648\n",
      "Train Accuracy: 0.5882% | Val Accuracy: 1.1765%\n",
      "Epoch 9/10 | Train Loss: 160.2356 | Val Loss: 158.4844\n",
      "Train Accuracy: 0.7843% | Val Accuracy: 1.0784%\n",
      "Early stopping. No improvement in validation accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = combined_model(inputs)\n",
    "            labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}% | Val Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping. No improvement in validation accuracy.\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(combined_model.state_dict(), 'resnet_with_attention.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 2.1142%\n",
      "Test Loss: 963.6000\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model (if not already loaded)\n",
    "combined_model.load_state_dict(torch.load('resnet_with_attention.pth'))\n",
    "combined_model.to(device)\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        labels = labels.view(-1)  # Reshape to a 1D tensor if needed\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "flowers2",
   "language": "python",
   "name": "flowers2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
