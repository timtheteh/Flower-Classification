{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afefee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the Flowers102 dataset with RGB images\n",
    "train_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='val',\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='test',\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64  # You can adjust the batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize a pre-trained ResNet-18 model and move it to the GPU\n",
    "feature_extractor = models.resnet18(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2]).to(device)  # Remove the classification layer\n",
    "\n",
    "# Define the prototype-based few-shot learner\n",
    "class FewShotLearner(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes):\n",
    "        super(FewShotLearner, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.num_classes = num_classes\n",
    "        self.out_size = self.calculate_output_size()\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, self.out_size).to(device))\n",
    "\n",
    "    def calculate_output_size(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "            features = self.feature_extractor(dummy_input)\n",
    "            return features.view(features.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        dists = torch.cdist(features, self.prototypes)\n",
    "        return -dists\n",
    "\n",
    "# Create a few-shot learner\n",
    "num_classes = 102\n",
    "few_shot_learner = FewShotLearner(feature_extractor, num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(few_shot_learner.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f9e8d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 3.4521, Train Accuracy: 30.49%, Validation Loss: 4.3229, Validation Accuracy: 6.96%\n",
      "Epoch [2/100], Train Loss: 2.7136, Train Accuracy: 65.20%, Validation Loss: 4.0851, Validation Accuracy: 11.47%\n",
      "Epoch [3/100], Train Loss: 2.0786, Train Accuracy: 88.82%, Validation Loss: 3.9000, Validation Accuracy: 17.45%\n",
      "Epoch [4/100], Train Loss: 1.5777, Train Accuracy: 97.45%, Validation Loss: 3.7423, Validation Accuracy: 22.55%\n",
      "Epoch [5/100], Train Loss: 1.1804, Train Accuracy: 99.41%, Validation Loss: 3.6119, Validation Accuracy: 26.57%\n",
      "Epoch [6/100], Train Loss: 0.8789, Train Accuracy: 100.00%, Validation Loss: 3.5197, Validation Accuracy: 30.20%\n",
      "Epoch [7/100], Train Loss: 0.6673, Train Accuracy: 100.00%, Validation Loss: 3.4501, Validation Accuracy: 32.75%\n",
      "Epoch [8/100], Train Loss: 0.5278, Train Accuracy: 100.00%, Validation Loss: 3.3923, Validation Accuracy: 33.63%\n",
      "Epoch [9/100], Train Loss: 0.4249, Train Accuracy: 100.00%, Validation Loss: 3.3480, Validation Accuracy: 35.20%\n",
      "Epoch [10/100], Train Loss: 0.3387, Train Accuracy: 100.00%, Validation Loss: 3.3105, Validation Accuracy: 36.96%\n",
      "Epoch [11/100], Train Loss: 0.2803, Train Accuracy: 100.00%, Validation Loss: 3.2784, Validation Accuracy: 38.24%\n",
      "Epoch [12/100], Train Loss: 0.2389, Train Accuracy: 100.00%, Validation Loss: 3.2473, Validation Accuracy: 39.51%\n",
      "Epoch [13/100], Train Loss: 0.2135, Train Accuracy: 100.00%, Validation Loss: 3.2250, Validation Accuracy: 40.10%\n",
      "Epoch [14/100], Train Loss: 0.1889, Train Accuracy: 100.00%, Validation Loss: 3.2081, Validation Accuracy: 40.20%\n",
      "Epoch [15/100], Train Loss: 0.1690, Train Accuracy: 100.00%, Validation Loss: 3.1883, Validation Accuracy: 40.49%\n",
      "Epoch [16/100], Train Loss: 0.1519, Train Accuracy: 100.00%, Validation Loss: 3.1691, Validation Accuracy: 41.57%\n",
      "Epoch [17/100], Train Loss: 0.1412, Train Accuracy: 100.00%, Validation Loss: 3.1572, Validation Accuracy: 41.86%\n",
      "Epoch [18/100], Train Loss: 0.1275, Train Accuracy: 100.00%, Validation Loss: 3.1389, Validation Accuracy: 42.06%\n",
      "Epoch [19/100], Train Loss: 0.1195, Train Accuracy: 100.00%, Validation Loss: 3.1254, Validation Accuracy: 42.25%\n",
      "Epoch [20/100], Train Loss: 0.1105, Train Accuracy: 100.00%, Validation Loss: 3.1134, Validation Accuracy: 42.35%\n",
      "Epoch [21/100], Train Loss: 0.1023, Train Accuracy: 100.00%, Validation Loss: 3.1018, Validation Accuracy: 42.65%\n",
      "Epoch [22/100], Train Loss: 0.0971, Train Accuracy: 100.00%, Validation Loss: 3.0976, Validation Accuracy: 42.55%\n",
      "Epoch [23/100], Train Loss: 0.0896, Train Accuracy: 100.00%, Validation Loss: 3.0863, Validation Accuracy: 42.55%\n",
      "Epoch [24/100], Train Loss: 0.0895, Train Accuracy: 100.00%, Validation Loss: 3.0771, Validation Accuracy: 42.55%\n",
      "Epoch [25/100], Train Loss: 0.0851, Train Accuracy: 100.00%, Validation Loss: 3.0714, Validation Accuracy: 42.75%\n",
      "Epoch [26/100], Train Loss: 0.0785, Train Accuracy: 100.00%, Validation Loss: 3.0609, Validation Accuracy: 43.04%\n",
      "Epoch [27/100], Train Loss: 0.0749, Train Accuracy: 100.00%, Validation Loss: 3.0505, Validation Accuracy: 43.53%\n",
      "Epoch [28/100], Train Loss: 0.0701, Train Accuracy: 100.00%, Validation Loss: 3.0463, Validation Accuracy: 43.43%\n",
      "Epoch [29/100], Train Loss: 0.0674, Train Accuracy: 100.00%, Validation Loss: 3.0388, Validation Accuracy: 43.73%\n",
      "Epoch [30/100], Train Loss: 0.0644, Train Accuracy: 100.00%, Validation Loss: 3.0324, Validation Accuracy: 43.24%\n",
      "Epoch [31/100], Train Loss: 0.0611, Train Accuracy: 100.00%, Validation Loss: 3.0275, Validation Accuracy: 44.12%\n",
      "Epoch [32/100], Train Loss: 0.0604, Train Accuracy: 100.00%, Validation Loss: 3.0232, Validation Accuracy: 44.02%\n",
      "Epoch [33/100], Train Loss: 0.0582, Train Accuracy: 100.00%, Validation Loss: 3.0182, Validation Accuracy: 44.41%\n",
      "Epoch [34/100], Train Loss: 0.0550, Train Accuracy: 100.00%, Validation Loss: 3.0119, Validation Accuracy: 44.71%\n",
      "Epoch [35/100], Train Loss: 0.0553, Train Accuracy: 100.00%, Validation Loss: 3.0060, Validation Accuracy: 44.41%\n",
      "Epoch [36/100], Train Loss: 0.0532, Train Accuracy: 100.00%, Validation Loss: 3.0000, Validation Accuracy: 44.41%\n",
      "Epoch [37/100], Train Loss: 0.0507, Train Accuracy: 100.00%, Validation Loss: 2.9965, Validation Accuracy: 44.80%\n",
      "Epoch [38/100], Train Loss: 0.0501, Train Accuracy: 100.00%, Validation Loss: 2.9907, Validation Accuracy: 45.10%\n",
      "Epoch [39/100], Train Loss: 0.0476, Train Accuracy: 100.00%, Validation Loss: 2.9853, Validation Accuracy: 45.49%\n",
      "Epoch [40/100], Train Loss: 0.0445, Train Accuracy: 100.00%, Validation Loss: 2.9808, Validation Accuracy: 45.39%\n",
      "Epoch [41/100], Train Loss: 0.0450, Train Accuracy: 100.00%, Validation Loss: 2.9778, Validation Accuracy: 45.10%\n",
      "Epoch [42/100], Train Loss: 0.0430, Train Accuracy: 100.00%, Validation Loss: 2.9716, Validation Accuracy: 44.90%\n",
      "Epoch [43/100], Train Loss: 0.0423, Train Accuracy: 100.00%, Validation Loss: 2.9702, Validation Accuracy: 45.49%\n",
      "Epoch [44/100], Train Loss: 0.0401, Train Accuracy: 100.00%, Validation Loss: 2.9652, Validation Accuracy: 45.59%\n",
      "Epoch [45/100], Train Loss: 0.0398, Train Accuracy: 100.00%, Validation Loss: 2.9634, Validation Accuracy: 45.88%\n",
      "Epoch [46/100], Train Loss: 0.0410, Train Accuracy: 100.00%, Validation Loss: 2.9584, Validation Accuracy: 45.88%\n",
      "Epoch [47/100], Train Loss: 0.0382, Train Accuracy: 100.00%, Validation Loss: 2.9561, Validation Accuracy: 45.98%\n",
      "Epoch [48/100], Train Loss: 0.0366, Train Accuracy: 100.00%, Validation Loss: 2.9526, Validation Accuracy: 46.27%\n",
      "Epoch [49/100], Train Loss: 0.0367, Train Accuracy: 100.00%, Validation Loss: 2.9486, Validation Accuracy: 45.88%\n",
      "Epoch [50/100], Train Loss: 0.0353, Train Accuracy: 100.00%, Validation Loss: 2.9449, Validation Accuracy: 46.47%\n",
      "Epoch [51/100], Train Loss: 0.0356, Train Accuracy: 100.00%, Validation Loss: 2.9407, Validation Accuracy: 46.37%\n",
      "Epoch [52/100], Train Loss: 0.0343, Train Accuracy: 100.00%, Validation Loss: 2.9381, Validation Accuracy: 46.47%\n",
      "Epoch [53/100], Train Loss: 0.0336, Train Accuracy: 100.00%, Validation Loss: 2.9343, Validation Accuracy: 46.47%\n",
      "Epoch [54/100], Train Loss: 0.0321, Train Accuracy: 100.00%, Validation Loss: 2.9326, Validation Accuracy: 46.57%\n",
      "Epoch [55/100], Train Loss: 0.0319, Train Accuracy: 100.00%, Validation Loss: 2.9317, Validation Accuracy: 46.27%\n",
      "Epoch [56/100], Train Loss: 0.0320, Train Accuracy: 100.00%, Validation Loss: 2.9278, Validation Accuracy: 46.76%\n",
      "Epoch [57/100], Train Loss: 0.0308, Train Accuracy: 100.00%, Validation Loss: 2.9240, Validation Accuracy: 46.96%\n",
      "Epoch [58/100], Train Loss: 0.0302, Train Accuracy: 100.00%, Validation Loss: 2.9205, Validation Accuracy: 46.57%\n",
      "Epoch [59/100], Train Loss: 0.0297, Train Accuracy: 100.00%, Validation Loss: 2.9175, Validation Accuracy: 46.57%\n",
      "Epoch [60/100], Train Loss: 0.0295, Train Accuracy: 100.00%, Validation Loss: 2.9169, Validation Accuracy: 47.06%\n",
      "Epoch [61/100], Train Loss: 0.0287, Train Accuracy: 100.00%, Validation Loss: 2.9106, Validation Accuracy: 47.45%\n",
      "Epoch [62/100], Train Loss: 0.0275, Train Accuracy: 100.00%, Validation Loss: 2.9079, Validation Accuracy: 47.45%\n",
      "Epoch [63/100], Train Loss: 0.0270, Train Accuracy: 100.00%, Validation Loss: 2.9056, Validation Accuracy: 47.94%\n",
      "Epoch [64/100], Train Loss: 0.0268, Train Accuracy: 100.00%, Validation Loss: 2.9045, Validation Accuracy: 47.25%\n",
      "Epoch [65/100], Train Loss: 0.0264, Train Accuracy: 100.00%, Validation Loss: 2.9013, Validation Accuracy: 47.35%\n",
      "Epoch [66/100], Train Loss: 0.0267, Train Accuracy: 100.00%, Validation Loss: 2.9010, Validation Accuracy: 47.25%\n",
      "Epoch [67/100], Train Loss: 0.0255, Train Accuracy: 100.00%, Validation Loss: 2.9004, Validation Accuracy: 47.75%\n",
      "Epoch [68/100], Train Loss: 0.0249, Train Accuracy: 100.00%, Validation Loss: 2.8954, Validation Accuracy: 47.75%\n",
      "Early stopping - no improvement in validation accuracy for 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs with no improvement to wait\n",
    "best_val_accuracy = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    few_shot_learner.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = few_shot_learner(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    few_shot_learner.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = few_shot_learner(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print('Early stopping - no improvement in validation accuracy for {} epochs.'.format(patience))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "807a749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.0128, Test Accuracy: 44.36%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test set\n",
    "few_shot_learner.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = few_shot_learner(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322c024",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f9515a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),  # Randomly crop and resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "        transforms.RandomRotation(30),  # Randomly rotate by up to 30 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust brightness, contrast, saturation, and hue\n",
    "        transforms.RandomGrayscale(p=0.2),  # Convert to grayscale with a 20% probability\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random affine transformation\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),  # Random perspective transformation\n",
    "        transforms.RandomVerticalFlip(),  # Randomly flip vertically\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),  # Resize for validation and test\n",
    "        transforms.CenterCrop(224),  # Center crop for validation and test\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),  # Resize for validation and test\n",
    "        transforms.CenterCrop(224),  # Center crop for validation and test\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Apply the data augmentation transformations to your datasets\n",
    "train_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    transform=data_transforms['train'],  # Use data augmentation for training\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='val',\n",
    "    transform=data_transforms['val'],  # Use validation data transformations\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='test',\n",
    "    transform=data_transforms['test'],  # Use test data transformations\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64  # You can adjust the batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize a pre-trained ResNet-18 model and move it to the GPU\n",
    "feature_extractor = models.resnet18(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2]).to(device)  # Remove the classification layer\n",
    "\n",
    "# Define the prototype-based few-shot learner\n",
    "class FewShotLearner(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes):\n",
    "        super(FewShotLearner, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.num_classes = num_classes\n",
    "        self.out_size = self.calculate_output_size()\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, self.out_size).to(device))\n",
    "\n",
    "    def calculate_output_size(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "            features = self.feature_extractor(dummy_input)\n",
    "            return features.view(features.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        dists = torch.cdist(features, self.prototypes)\n",
    "        return -dists\n",
    "\n",
    "# Create a few-shot learner\n",
    "num_classes = 102\n",
    "few_shot_learner = FewShotLearner(feature_extractor, num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(few_shot_learner.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fdf941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 5.0106, Train Accuracy: 0.59%, Validation Loss: 4.9233, Validation Accuracy: 1.08%\n",
      "Epoch [2/100], Train Loss: 4.8420, Train Accuracy: 1.37%, Validation Loss: 4.8211, Validation Accuracy: 1.67%\n",
      "Epoch [3/100], Train Loss: 4.7514, Train Accuracy: 1.57%, Validation Loss: 4.7090, Validation Accuracy: 2.06%\n",
      "Epoch [4/100], Train Loss: 4.6535, Train Accuracy: 1.86%, Validation Loss: 4.5909, Validation Accuracy: 2.06%\n",
      "Epoch [5/100], Train Loss: 4.5335, Train Accuracy: 2.84%, Validation Loss: 4.4778, Validation Accuracy: 4.02%\n",
      "Epoch [6/100], Train Loss: 4.4414, Train Accuracy: 4.12%, Validation Loss: 4.3721, Validation Accuracy: 6.37%\n",
      "Epoch [7/100], Train Loss: 4.3081, Train Accuracy: 6.67%, Validation Loss: 4.2518, Validation Accuracy: 8.33%\n",
      "Epoch [8/100], Train Loss: 4.2044, Train Accuracy: 9.12%, Validation Loss: 4.1478, Validation Accuracy: 9.12%\n",
      "Epoch [9/100], Train Loss: 4.0434, Train Accuracy: 14.22%, Validation Loss: 4.0279, Validation Accuracy: 12.55%\n",
      "Epoch [10/100], Train Loss: 3.9517, Train Accuracy: 14.22%, Validation Loss: 3.9114, Validation Accuracy: 15.59%\n",
      "Epoch [11/100], Train Loss: 3.8737, Train Accuracy: 16.67%, Validation Loss: 3.7760, Validation Accuracy: 18.33%\n",
      "Epoch [12/100], Train Loss: 3.7726, Train Accuracy: 18.63%, Validation Loss: 3.6513, Validation Accuracy: 22.16%\n",
      "Epoch [13/100], Train Loss: 3.7087, Train Accuracy: 21.76%, Validation Loss: 3.5348, Validation Accuracy: 23.92%\n",
      "Epoch [14/100], Train Loss: 3.5121, Train Accuracy: 26.18%, Validation Loss: 3.4072, Validation Accuracy: 27.94%\n",
      "Epoch [15/100], Train Loss: 3.4314, Train Accuracy: 28.33%, Validation Loss: 3.2924, Validation Accuracy: 32.06%\n",
      "Epoch [16/100], Train Loss: 3.3720, Train Accuracy: 28.92%, Validation Loss: 3.1822, Validation Accuracy: 34.90%\n",
      "Epoch [17/100], Train Loss: 3.2595, Train Accuracy: 33.33%, Validation Loss: 3.0605, Validation Accuracy: 37.65%\n",
      "Epoch [18/100], Train Loss: 3.2243, Train Accuracy: 32.94%, Validation Loss: 2.9728, Validation Accuracy: 41.57%\n",
      "Epoch [19/100], Train Loss: 3.0674, Train Accuracy: 39.41%, Validation Loss: 2.8857, Validation Accuracy: 42.75%\n",
      "Epoch [20/100], Train Loss: 3.0498, Train Accuracy: 37.75%, Validation Loss: 2.7877, Validation Accuracy: 44.61%\n",
      "Epoch [21/100], Train Loss: 2.9177, Train Accuracy: 39.02%, Validation Loss: 2.7158, Validation Accuracy: 47.25%\n",
      "Epoch [22/100], Train Loss: 2.8445, Train Accuracy: 43.04%, Validation Loss: 2.6340, Validation Accuracy: 47.94%\n",
      "Epoch [23/100], Train Loss: 2.8099, Train Accuracy: 43.14%, Validation Loss: 2.5352, Validation Accuracy: 49.90%\n",
      "Epoch [24/100], Train Loss: 2.6924, Train Accuracy: 47.16%, Validation Loss: 2.4932, Validation Accuracy: 51.76%\n",
      "Epoch [25/100], Train Loss: 2.6258, Train Accuracy: 49.31%, Validation Loss: 2.3917, Validation Accuracy: 52.65%\n",
      "Epoch [26/100], Train Loss: 2.6112, Train Accuracy: 46.86%, Validation Loss: 2.3532, Validation Accuracy: 54.61%\n",
      "Epoch [27/100], Train Loss: 2.5817, Train Accuracy: 48.33%, Validation Loss: 2.2682, Validation Accuracy: 55.20%\n",
      "Epoch [28/100], Train Loss: 2.5112, Train Accuracy: 50.69%, Validation Loss: 2.1832, Validation Accuracy: 56.96%\n",
      "Epoch [29/100], Train Loss: 2.4384, Train Accuracy: 52.06%, Validation Loss: 2.1426, Validation Accuracy: 57.84%\n",
      "Epoch [30/100], Train Loss: 2.3607, Train Accuracy: 51.96%, Validation Loss: 2.0729, Validation Accuracy: 59.12%\n",
      "Epoch [31/100], Train Loss: 2.3418, Train Accuracy: 53.33%, Validation Loss: 2.0294, Validation Accuracy: 59.61%\n",
      "Epoch [32/100], Train Loss: 2.2845, Train Accuracy: 54.51%, Validation Loss: 1.9796, Validation Accuracy: 60.10%\n",
      "Epoch [33/100], Train Loss: 2.2590, Train Accuracy: 54.80%, Validation Loss: 1.9349, Validation Accuracy: 61.37%\n",
      "Epoch [34/100], Train Loss: 2.2337, Train Accuracy: 54.12%, Validation Loss: 1.8871, Validation Accuracy: 62.16%\n",
      "Epoch [35/100], Train Loss: 2.1377, Train Accuracy: 57.25%, Validation Loss: 1.8524, Validation Accuracy: 63.24%\n",
      "Epoch [36/100], Train Loss: 2.0895, Train Accuracy: 59.31%, Validation Loss: 1.7802, Validation Accuracy: 65.29%\n",
      "Epoch [37/100], Train Loss: 2.0420, Train Accuracy: 59.61%, Validation Loss: 1.7609, Validation Accuracy: 63.53%\n",
      "Epoch [38/100], Train Loss: 1.9974, Train Accuracy: 60.78%, Validation Loss: 1.7143, Validation Accuracy: 66.57%\n",
      "Epoch [39/100], Train Loss: 2.0032, Train Accuracy: 61.08%, Validation Loss: 1.6590, Validation Accuracy: 67.35%\n",
      "Epoch [40/100], Train Loss: 1.9255, Train Accuracy: 63.04%, Validation Loss: 1.6092, Validation Accuracy: 69.22%\n",
      "Epoch [41/100], Train Loss: 1.9268, Train Accuracy: 60.88%, Validation Loss: 1.6085, Validation Accuracy: 67.84%\n",
      "Epoch [42/100], Train Loss: 1.8480, Train Accuracy: 64.02%, Validation Loss: 1.5634, Validation Accuracy: 70.00%\n",
      "Epoch [43/100], Train Loss: 1.8139, Train Accuracy: 64.22%, Validation Loss: 1.5211, Validation Accuracy: 70.49%\n",
      "Epoch [44/100], Train Loss: 1.7838, Train Accuracy: 66.47%, Validation Loss: 1.5153, Validation Accuracy: 69.90%\n",
      "Epoch [45/100], Train Loss: 1.7563, Train Accuracy: 65.00%, Validation Loss: 1.4709, Validation Accuracy: 70.29%\n",
      "Epoch [46/100], Train Loss: 1.7283, Train Accuracy: 65.98%, Validation Loss: 1.4393, Validation Accuracy: 71.86%\n",
      "Epoch [47/100], Train Loss: 1.7155, Train Accuracy: 66.67%, Validation Loss: 1.4044, Validation Accuracy: 72.45%\n",
      "Epoch [48/100], Train Loss: 1.6753, Train Accuracy: 66.47%, Validation Loss: 1.4002, Validation Accuracy: 73.24%\n",
      "Epoch [49/100], Train Loss: 1.5709, Train Accuracy: 69.71%, Validation Loss: 1.3803, Validation Accuracy: 72.65%\n",
      "Epoch [50/100], Train Loss: 1.6388, Train Accuracy: 66.76%, Validation Loss: 1.3248, Validation Accuracy: 74.61%\n",
      "Epoch [51/100], Train Loss: 1.5846, Train Accuracy: 70.88%, Validation Loss: 1.3160, Validation Accuracy: 73.92%\n",
      "Epoch [52/100], Train Loss: 1.5748, Train Accuracy: 70.29%, Validation Loss: 1.2832, Validation Accuracy: 75.10%\n",
      "Epoch [53/100], Train Loss: 1.5477, Train Accuracy: 67.75%, Validation Loss: 1.2602, Validation Accuracy: 74.71%\n",
      "Epoch [54/100], Train Loss: 1.4774, Train Accuracy: 71.67%, Validation Loss: 1.2288, Validation Accuracy: 75.10%\n",
      "Epoch [55/100], Train Loss: 1.5156, Train Accuracy: 70.69%, Validation Loss: 1.2171, Validation Accuracy: 73.73%\n",
      "Epoch [56/100], Train Loss: 1.5067, Train Accuracy: 69.90%, Validation Loss: 1.1862, Validation Accuracy: 76.57%\n",
      "Epoch [57/100], Train Loss: 1.4184, Train Accuracy: 71.76%, Validation Loss: 1.1733, Validation Accuracy: 76.08%\n",
      "Epoch [58/100], Train Loss: 1.4859, Train Accuracy: 69.51%, Validation Loss: 1.1668, Validation Accuracy: 75.49%\n",
      "Epoch [59/100], Train Loss: 1.3560, Train Accuracy: 72.75%, Validation Loss: 1.1527, Validation Accuracy: 75.39%\n",
      "Epoch [60/100], Train Loss: 1.3864, Train Accuracy: 72.65%, Validation Loss: 1.0900, Validation Accuracy: 77.65%\n",
      "Epoch [61/100], Train Loss: 1.3556, Train Accuracy: 73.92%, Validation Loss: 1.1166, Validation Accuracy: 76.27%\n",
      "Epoch [62/100], Train Loss: 1.3070, Train Accuracy: 74.31%, Validation Loss: 1.1010, Validation Accuracy: 77.06%\n",
      "Epoch [63/100], Train Loss: 1.3903, Train Accuracy: 71.37%, Validation Loss: 1.0956, Validation Accuracy: 77.94%\n",
      "Epoch [64/100], Train Loss: 1.3263, Train Accuracy: 73.92%, Validation Loss: 1.0870, Validation Accuracy: 77.84%\n",
      "Epoch [65/100], Train Loss: 1.2664, Train Accuracy: 75.10%, Validation Loss: 1.0820, Validation Accuracy: 77.06%\n",
      "Epoch [66/100], Train Loss: 1.2971, Train Accuracy: 73.73%, Validation Loss: 1.0407, Validation Accuracy: 78.53%\n",
      "Epoch [67/100], Train Loss: 1.2770, Train Accuracy: 75.00%, Validation Loss: 1.0047, Validation Accuracy: 78.53%\n",
      "Epoch [68/100], Train Loss: 1.2373, Train Accuracy: 75.98%, Validation Loss: 1.0148, Validation Accuracy: 78.82%\n",
      "Epoch [69/100], Train Loss: 1.2340, Train Accuracy: 76.37%, Validation Loss: 1.0083, Validation Accuracy: 78.24%\n",
      "Epoch [70/100], Train Loss: 1.1912, Train Accuracy: 74.80%, Validation Loss: 0.9847, Validation Accuracy: 79.12%\n",
      "Epoch [71/100], Train Loss: 1.1946, Train Accuracy: 76.57%, Validation Loss: 0.9884, Validation Accuracy: 78.24%\n",
      "Epoch [72/100], Train Loss: 1.1855, Train Accuracy: 77.84%, Validation Loss: 0.9399, Validation Accuracy: 80.69%\n",
      "Epoch [73/100], Train Loss: 1.2066, Train Accuracy: 75.88%, Validation Loss: 0.9528, Validation Accuracy: 79.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Train Loss: 1.1265, Train Accuracy: 79.80%, Validation Loss: 0.9453, Validation Accuracy: 80.29%\n",
      "Epoch [75/100], Train Loss: 1.1724, Train Accuracy: 75.98%, Validation Loss: 0.9495, Validation Accuracy: 79.22%\n",
      "Epoch [76/100], Train Loss: 1.1749, Train Accuracy: 75.49%, Validation Loss: 0.9413, Validation Accuracy: 79.71%\n",
      "Epoch [77/100], Train Loss: 1.0950, Train Accuracy: 79.31%, Validation Loss: 0.9033, Validation Accuracy: 80.69%\n",
      "Early stopping - no improvement in validation accuracy for 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs with no improvement to wait\n",
    "best_val_accuracy = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    few_shot_learner.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = few_shot_learner(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    few_shot_learner.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = few_shot_learner(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print('Early stopping - no improvement in validation accuracy for {} epochs.'.format(patience))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "382a946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9899, Test Accuracy: 78.61%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test set\n",
    "few_shot_learner.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = few_shot_learner(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12ad8d",
   "metadata": {},
   "source": [
    "### Resize to (500,500) -> Edit first layer of resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e668534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((500, 500)),   # Randomly crop and resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "        transforms.RandomRotation(30),  # Randomly rotate by up to 30 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust brightness, contrast, saturation, and hue\n",
    "        transforms.RandomGrayscale(p=0.2),  # Convert to grayscale with a 20% probability\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random affine transformation\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),  # Random perspective transformation\n",
    "        transforms.RandomVerticalFlip(),  # Randomly flip vertically\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((500, 500)),  # Resize for validation and test\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((500, 500)),  # Resize for validation and test\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Apply the data augmentation transformations to your datasets\n",
    "train_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    transform=data_transforms['train'],  # Use data augmentation for training\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='val',\n",
    "    transform=data_transforms['val'],  # Use validation data transformations\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = Flowers102(\n",
    "    root='./data',\n",
    "    split='test',\n",
    "    transform=data_transforms['test'],  # Use test data transformations\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64  # You can adjust the batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize a pre-trained ResNet-18 model and move it to the GPU\n",
    "feature_extractor = models.resnet18(pretrained=True)\n",
    "# Modify the first convolutional layer to accept 3-channel (RGB) images of size 500x500\n",
    "# You need to change the in_channels argument to 3 and kernel_size to 7\n",
    "feature_extractor.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2]).to(device)  # Remove the classification layer\n",
    "\n",
    "# Define the prototype-based few-shot learner\n",
    "class FewShotLearner(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes):\n",
    "        super(FewShotLearner, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.num_classes = num_classes\n",
    "        self.out_size = self.calculate_output_size()\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, self.out_size).to(device))\n",
    "\n",
    "    def calculate_output_size(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 500, 500).to(device)\n",
    "            features = self.feature_extractor(dummy_input)\n",
    "            return features.view(features.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        dists = torch.cdist(features, self.prototypes)\n",
    "        return -dists\n",
    "\n",
    "# Create a few-shot learner\n",
    "num_classes = 102\n",
    "few_shot_learner = FewShotLearner(feature_extractor, num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(few_shot_learner.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae2a9061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 5.0164, Train Accuracy: 1.27%, Validation Loss: 5.0336, Validation Accuracy: 0.98%\n",
      "Epoch [2/100], Train Loss: 4.9266, Train Accuracy: 1.18%, Validation Loss: 4.9019, Validation Accuracy: 1.67%\n",
      "Epoch [3/100], Train Loss: 4.8395, Train Accuracy: 1.47%, Validation Loss: 4.8679, Validation Accuracy: 1.27%\n",
      "Epoch [4/100], Train Loss: 4.7731, Train Accuracy: 2.16%, Validation Loss: 4.7925, Validation Accuracy: 2.55%\n",
      "Epoch [5/100], Train Loss: 4.7336, Train Accuracy: 1.18%, Validation Loss: 4.7609, Validation Accuracy: 2.06%\n",
      "Epoch [6/100], Train Loss: 4.6496, Train Accuracy: 2.06%, Validation Loss: 4.6849, Validation Accuracy: 3.04%\n",
      "Epoch [7/100], Train Loss: 4.5925, Train Accuracy: 1.57%, Validation Loss: 4.6285, Validation Accuracy: 3.53%\n",
      "Epoch [8/100], Train Loss: 4.5013, Train Accuracy: 3.63%, Validation Loss: 4.5531, Validation Accuracy: 3.63%\n",
      "Epoch [9/100], Train Loss: 4.4736, Train Accuracy: 3.63%, Validation Loss: 4.4854, Validation Accuracy: 4.12%\n",
      "Epoch [10/100], Train Loss: 4.3946, Train Accuracy: 4.22%, Validation Loss: 4.4457, Validation Accuracy: 4.71%\n",
      "Epoch [11/100], Train Loss: 4.3546, Train Accuracy: 6.86%, Validation Loss: 4.3452, Validation Accuracy: 6.18%\n",
      "Epoch [12/100], Train Loss: 4.2744, Train Accuracy: 7.55%, Validation Loss: 4.3318, Validation Accuracy: 5.10%\n",
      "Epoch [13/100], Train Loss: 4.2162, Train Accuracy: 8.53%, Validation Loss: 4.2276, Validation Accuracy: 7.35%\n",
      "Epoch [14/100], Train Loss: 4.1786, Train Accuracy: 9.90%, Validation Loss: 4.1360, Validation Accuracy: 9.71%\n",
      "Epoch [15/100], Train Loss: 4.1559, Train Accuracy: 8.92%, Validation Loss: 4.0846, Validation Accuracy: 9.51%\n",
      "Epoch [16/100], Train Loss: 4.0317, Train Accuracy: 11.76%, Validation Loss: 4.0311, Validation Accuracy: 11.18%\n",
      "Epoch [17/100], Train Loss: 4.0474, Train Accuracy: 11.76%, Validation Loss: 3.9713, Validation Accuracy: 12.65%\n",
      "Epoch [18/100], Train Loss: 3.9549, Train Accuracy: 14.22%, Validation Loss: 3.9358, Validation Accuracy: 12.94%\n",
      "Epoch [19/100], Train Loss: 3.9429, Train Accuracy: 14.22%, Validation Loss: 3.8666, Validation Accuracy: 15.59%\n",
      "Epoch [20/100], Train Loss: 3.8691, Train Accuracy: 15.78%, Validation Loss: 3.8153, Validation Accuracy: 16.76%\n",
      "Epoch [21/100], Train Loss: 3.7699, Train Accuracy: 18.63%, Validation Loss: 3.7751, Validation Accuracy: 17.25%\n",
      "Epoch [22/100], Train Loss: 3.7393, Train Accuracy: 19.71%, Validation Loss: 3.6960, Validation Accuracy: 18.82%\n",
      "Epoch [23/100], Train Loss: 3.7518, Train Accuracy: 17.94%, Validation Loss: 3.6577, Validation Accuracy: 19.02%\n",
      "Epoch [24/100], Train Loss: 3.6635, Train Accuracy: 19.22%, Validation Loss: 3.6148, Validation Accuracy: 20.00%\n",
      "Epoch [25/100], Train Loss: 3.6291, Train Accuracy: 21.67%, Validation Loss: 3.5749, Validation Accuracy: 22.84%\n",
      "Epoch [26/100], Train Loss: 3.5556, Train Accuracy: 23.63%, Validation Loss: 3.5373, Validation Accuracy: 22.45%\n",
      "Epoch [27/100], Train Loss: 3.5605, Train Accuracy: 23.33%, Validation Loss: 3.4920, Validation Accuracy: 24.51%\n",
      "Epoch [28/100], Train Loss: 3.5203, Train Accuracy: 24.31%, Validation Loss: 3.4610, Validation Accuracy: 23.73%\n",
      "Epoch [29/100], Train Loss: 3.4806, Train Accuracy: 25.59%, Validation Loss: 3.4330, Validation Accuracy: 23.73%\n",
      "Epoch [30/100], Train Loss: 3.4561, Train Accuracy: 27.35%, Validation Loss: 3.3510, Validation Accuracy: 27.55%\n",
      "Epoch [31/100], Train Loss: 3.3856, Train Accuracy: 27.94%, Validation Loss: 3.3354, Validation Accuracy: 27.16%\n",
      "Epoch [32/100], Train Loss: 3.3380, Train Accuracy: 27.75%, Validation Loss: 3.2912, Validation Accuracy: 28.14%\n",
      "Epoch [33/100], Train Loss: 3.3256, Train Accuracy: 29.80%, Validation Loss: 3.2488, Validation Accuracy: 29.22%\n",
      "Epoch [34/100], Train Loss: 3.2877, Train Accuracy: 30.69%, Validation Loss: 3.2570, Validation Accuracy: 27.94%\n",
      "Epoch [35/100], Train Loss: 3.2735, Train Accuracy: 30.20%, Validation Loss: 3.1844, Validation Accuracy: 30.88%\n",
      "Epoch [36/100], Train Loss: 3.2382, Train Accuracy: 31.67%, Validation Loss: 3.1477, Validation Accuracy: 31.08%\n",
      "Epoch [37/100], Train Loss: 3.1533, Train Accuracy: 33.63%, Validation Loss: 3.1321, Validation Accuracy: 31.86%\n",
      "Epoch [38/100], Train Loss: 3.1273, Train Accuracy: 34.31%, Validation Loss: 3.1300, Validation Accuracy: 32.25%\n",
      "Epoch [39/100], Train Loss: 3.0751, Train Accuracy: 35.20%, Validation Loss: 3.0872, Validation Accuracy: 33.33%\n",
      "Epoch [40/100], Train Loss: 3.0448, Train Accuracy: 37.16%, Validation Loss: 3.0231, Validation Accuracy: 34.51%\n",
      "Epoch [41/100], Train Loss: 3.0379, Train Accuracy: 35.59%, Validation Loss: 3.0110, Validation Accuracy: 35.29%\n",
      "Epoch [42/100], Train Loss: 3.0215, Train Accuracy: 37.94%, Validation Loss: 2.9627, Validation Accuracy: 35.39%\n",
      "Epoch [43/100], Train Loss: 2.9844, Train Accuracy: 36.96%, Validation Loss: 2.9817, Validation Accuracy: 34.61%\n",
      "Epoch [44/100], Train Loss: 2.9973, Train Accuracy: 36.96%, Validation Loss: 2.9326, Validation Accuracy: 36.37%\n",
      "Epoch [45/100], Train Loss: 2.9244, Train Accuracy: 38.82%, Validation Loss: 2.9131, Validation Accuracy: 37.06%\n",
      "Epoch [46/100], Train Loss: 2.8483, Train Accuracy: 41.08%, Validation Loss: 2.8448, Validation Accuracy: 40.20%\n",
      "Epoch [47/100], Train Loss: 2.8707, Train Accuracy: 40.78%, Validation Loss: 2.8008, Validation Accuracy: 40.49%\n",
      "Epoch [48/100], Train Loss: 2.8033, Train Accuracy: 42.94%, Validation Loss: 2.7930, Validation Accuracy: 42.45%\n",
      "Epoch [49/100], Train Loss: 2.7764, Train Accuracy: 43.82%, Validation Loss: 2.7863, Validation Accuracy: 39.90%\n",
      "Epoch [50/100], Train Loss: 2.7561, Train Accuracy: 43.92%, Validation Loss: 2.7966, Validation Accuracy: 41.08%\n",
      "Epoch [51/100], Train Loss: 2.7279, Train Accuracy: 45.10%, Validation Loss: 2.7672, Validation Accuracy: 41.57%\n",
      "Epoch [52/100], Train Loss: 2.6983, Train Accuracy: 43.43%, Validation Loss: 2.7115, Validation Accuracy: 43.63%\n",
      "Epoch [53/100], Train Loss: 2.6737, Train Accuracy: 45.20%, Validation Loss: 2.7199, Validation Accuracy: 42.84%\n",
      "Epoch [54/100], Train Loss: 2.6608, Train Accuracy: 47.94%, Validation Loss: 2.6737, Validation Accuracy: 43.43%\n",
      "Epoch [55/100], Train Loss: 2.6319, Train Accuracy: 45.78%, Validation Loss: 2.6621, Validation Accuracy: 43.53%\n",
      "Epoch [56/100], Train Loss: 2.5958, Train Accuracy: 46.67%, Validation Loss: 2.6361, Validation Accuracy: 43.53%\n",
      "Epoch [57/100], Train Loss: 2.6046, Train Accuracy: 49.12%, Validation Loss: 2.6355, Validation Accuracy: 43.92%\n",
      "Epoch [58/100], Train Loss: 2.4783, Train Accuracy: 52.25%, Validation Loss: 2.6020, Validation Accuracy: 45.39%\n",
      "Epoch [59/100], Train Loss: 2.5771, Train Accuracy: 48.14%, Validation Loss: 2.5845, Validation Accuracy: 46.47%\n",
      "Epoch [60/100], Train Loss: 2.5172, Train Accuracy: 49.80%, Validation Loss: 2.5775, Validation Accuracy: 45.59%\n",
      "Epoch [61/100], Train Loss: 2.5218, Train Accuracy: 49.41%, Validation Loss: 2.5277, Validation Accuracy: 47.75%\n",
      "Epoch [62/100], Train Loss: 2.4984, Train Accuracy: 51.27%, Validation Loss: 2.5143, Validation Accuracy: 47.84%\n",
      "Epoch [63/100], Train Loss: 2.4920, Train Accuracy: 49.80%, Validation Loss: 2.4998, Validation Accuracy: 47.16%\n",
      "Epoch [64/100], Train Loss: 2.3786, Train Accuracy: 54.71%, Validation Loss: 2.4498, Validation Accuracy: 50.20%\n",
      "Epoch [65/100], Train Loss: 2.3742, Train Accuracy: 52.94%, Validation Loss: 2.4284, Validation Accuracy: 49.12%\n",
      "Epoch [66/100], Train Loss: 2.3817, Train Accuracy: 53.63%, Validation Loss: 2.4320, Validation Accuracy: 50.10%\n",
      "Epoch [67/100], Train Loss: 2.3384, Train Accuracy: 54.71%, Validation Loss: 2.4468, Validation Accuracy: 48.53%\n",
      "Epoch [68/100], Train Loss: 2.2567, Train Accuracy: 55.98%, Validation Loss: 2.3938, Validation Accuracy: 48.24%\n",
      "Epoch [69/100], Train Loss: 2.2900, Train Accuracy: 55.10%, Validation Loss: 2.3757, Validation Accuracy: 49.90%\n",
      "Early stopping - no improvement in validation accuracy for 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs with no improvement to wait\n",
    "best_val_accuracy = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    few_shot_learner.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = few_shot_learner(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    few_shot_learner.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = few_shot_learner(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print('Early stopping - no improvement in validation accuracy for {} epochs.'.format(patience))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cf69c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.5585, Test Accuracy: 44.61%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test set\n",
    "few_shot_learner.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = few_shot_learner(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798ca48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowers2",
   "language": "python",
   "name": "flowers2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
